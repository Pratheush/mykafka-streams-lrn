
Different Client APIs to interact with KAFKA::::

Two Basic APIs interacting Kafka i.e.
1. Producer API
2. Consumer API

Producer API responsible for producing or publishing data into Kakfa topic 
Consumer API responsible for consuming events or data from the Kafka topic and take action on those events.


In addition to  above basic APIs we have Two Advance CLient APIs and they are as follows::-
1. Connect API
2. Streams API

THe Kafka connect API further divided into two types :
(a). Source Connector Kafka Connect
(b). Sink Connector Kafka Connect.

(a). Source Connector Kafka Connect::-
Source Connector Kafka Connect is responsible for reading the data from an external data source and publish the data as events into the kafka topic

(b). Sink Connector Kafka Connect::-
Sink Connector Kafka Connect is responsible for extracting the data from the kafka topic and write it to a external data source.

2. Kafka Streams API::-
This API basically reads data from kafka topic and there can be many different things can be performed on data we can apply transformations, data-enrichments, branching the data into multiple data streams , aggregating the data, joining the data from multiple kafka topics and then writing it back to the kafka topic.


Kafka Streams is a Java Library which primary focuses on : 
 1. Data Enrichment
 2. Transforming of Data
 3. Aggregating the Data
 4. Joining Data from Multiple Kafka Topics

Kafka Streams API uses Functional Programming Style ::
 1. Lamdas
 2. Map, Filter, FlatMap operators similart to Modern Java Features

Kafka Streams API is build on top of Java 8


1.Kafka Streams Application initiates the stream processing by subscribing  one or multiple kafka-topics 
2. Act on the data from the Kafka-Topics like (aggregation, transforming, joining etc) once the processing completes on the data then we write the data back to the kafka-topic for the downstream applications which are interested 
in this data.

Behind the Scenes Streams API uses the producer and consumer API to read and write it back to Kafka Topic.

So HOw is this different from Consumer API?
Consumer Application built using Consumer API are stateless.

Kafka COnsumer App data Flow:
   1. Read the event
   2. PRocess the Event
   3. Move on to the next event.

Consumer apps are great for notifications or processing each event independent of each other.

COnsumer apps does not have an easy way to join or aggregate events.

-------------------------------------------------------------------------------------------------------------------------------------

Streams API use cases

1. Stateful and Stateless applictions
2. Stateless applications are similar to what we build using the Consumer API
   but the only difference is that Streams API uses functional Programming style which is different from Consumer API.

3. Stateful Operation:
	Stateful applications can be build using Kafka Streams API 
	1. Retail::
		(a). Calculating the total number of orders in realtime
		(b). calculating a total revenue made in realtime
	2. Entertainment:
		(a). Total number of tickets sold in realtime
		(b). Total revenue generated by a movie in realtime.

=============================================================================================================================================

Stream API Implementations::
1. Stream DSL
     (a). This is High level API (predefined operators available to build streaming logic)
     (b). Use OPerators to build your stream processing logic 
		operators are like map, flatMap etc.

2. Processor API
     (a). This is a low level API
     (b). Complex compared to Streams DSL
     (c). Streams DSL is built on top of Processor API


================================================================================================================================================

Kafka Stream Terminologies Topology & Processor

1. A Kafka stream processing application has a series of processors.

                 		   --------
  (Source Topic) Source Processor(1)	  |
			||		  |	
		        \/		  |	  |
 (Processing Logic)Stream Processor(2)	  |>>> Topology
		        ||		  |
			\/		  |
(Destination Topic) Sink Processor(3)	  |
				-----------	

Source Processor responsible reading from Source TOpic or Topics

Stream Processor where we have processing Logic which is responsible for acting on the data that read from the Kafka Topic. This could be aggregating the data or transforming the data or joining the data, this is where data-enrichment happens

Sink Processor Where we have Processed Data which is return into a Kafka-Topic 


This Concept of Designing the Kafka Streams application in this approach is basically a Directed Acyclic Graph (DAG)

THe collection of Processors in a Kafka Streams is called a Topology. so topology represents apprecations end to end flow....


KAFKA Streams Also has concept of SubTopology


						Source Processor
								|
						Stream Processor
							/		\
						  /		     \
	 Sink Processor	    	Sink Processor	
    	      |			 |
	Stream Processor    Stream Processor


Here Source Processor which is responsible for reading the data from Kafka Topic and THe Stream-Processor here Splits the messages into two Branches and writes it into two different Topics which is a Sink Processor here. And With Sink PRocessor there is Another Stream Processor attached which means the Sink-Processor for one topology act as a Source-Processor for another topology or another flow. This particular concept is called SubTopology. Here we have Two SubTopologies on top of the Actual Topology.


======================================================================================================================================================

How The Data Flows in A Kafka Streams Processing?

Here We have 3 Events A,B,C that present in Kafka-Topic
When the Source-Processor reads data from Kafka-Topic and it places a record in the streams Record Buffer and data is passed into the topology from the record-buffer one-by-one in the order it was received to all the nodes in the topology.
At any given point of time only one record gets processed in the topology.
With Subtopology in place this rule is applicable to each subtopology


  A,B,C>>>            		   --------
  (Source Topic) Source Processor(1)	  |
			||		  |	
		        \/		  |	  |
 (Processing Logic)Stream Processor(2)	  |>>> Topology
		        ||		  |
			\/		  |
(Destination Topic) Sink Processor(3)	  |
				-----------	


we have two options when it comes to building the source-processor
We can build Source-Processor as KStream Or KTable using the Streams-DSL

KStream:: 
KStream is an abstraction in Kafka Streams which holds each event in the Kafka Topic.

FOR EXAMPLE:::

		Kafka-Topic(Key,Value)
A,Apple    B,Ball,    C,Cat      D,Dog      A,Ant     B,Baby


Lets say we have a Kafka-topic which has all these records  in it.

Now we have KStream created using Kafka-Stream-DSL 
KStream gives you access to all the records in the Kafka-Topic
and Treats each independent of each-other .

KStream treats each event Independent of one another.

Each Event will be executed by the whole topology which includes Source-Processor, Stream-Processor, Sink-Processor.

Any new event is made available to the KStream.This is why KStream can be called as Record Stream or Log which represent everything thats happened.

Record Stream is Infinite which means KStream is Infinite. its going to constantly get the new records that's available in the Kafka-Topic or That's been pushed into the Kafka-Topic and its going to have the whole topology executed from that new record. 

Analogy :: Inserts into A DataBase Table because each and every row is independent of the other one.


========================================================================================================================



STEP 1: DOWNLOAD AND INSTALL KAFKA
https://dlcdn.apache.org/kafka/3.2.0/kafka_2.13-3.2.0.tgz

STEP 2: START THE KAFKA ENVIRONMENT
# Start the ZooKeeper service
C:\Users\RAMESH\Downloads\kafka>.\bin\windows\zookeeper-server-start.bat .\config\zookeeper.properties

# Start the Kafka broker service
C:\Users\RAMESH\Downloads\kafka>.\bin\windows\kafka-server-start.bat .\config\server.properties

STEP 3: CREATE A TOPIC TO STORE YOUR EVENTS
C:\Users\RAMESH\Downloads\kafka>.\bin\windows\kafka-topics.bat --create --topic topic_demo --bootstrap-server localhost:9092

STEP 4: WRITE SOME EVENTS INTO THE TOPIC
C:\Users\RAMESH\Downloads\kafka>.\bin\windows\kafka-console-producer.bat --topic topic_demo --bootstrap-server localhost:9092
>hello world
>topic demo


C:\Users\RAMESH\Downloads\kafka>.\bin\windows\kafka-console-producer.bat --topic topic_demo --broker-list localhost:9092
>hello world
>topic demo


SENDING BULK EVENTS OR MESSAGE INTO THE TOPIC 
C:\Users\RAMESH\Downloads\kafka>.\bin\windows\kafka-console-producer.bat --topic topic_demo --broker-list localhost:9092 </path/to/file_like_here_we_can_give_csv_file with_records_of_10000.csv





STEP 5:  READ THE EVENTS
C:\Users\RAMESH\Downloads\kafka>.\bin\windows\kafka-console-consumer.bat --topic topic_demo --from-beginning --bootstrap-server localhost:9092
hello world
topic demo


CONSUMING OR READING EVENTS OR MESSAGE BUT NOT FROM FROM BEGINNING
C:\Users\RAMESH\Downloads\kafka>.\bin\windows\kafka-console-consumer.bat --topic topic_demo --bootstrap-server localhost:9092



creating topic and also partitions and replication-factor
.\bin\windows\kafka-topics.bat --create --topic topic_name --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1

how many copy you want to keep i.e. replication factor since we are running on a single broker so lets keep the replication factor as 1
i want to keep single clone for the broker that is why replication-factor is 1


TO LIST THE TOPIC LIST THAT WAS CREATED
.\bin\windows\kafka-topics.bat --bootstrap-server localhost:9092 --list


TO GET THE DETAILS OF CREATED TOPIC LIKE REPLICATION-FACTOR , PARTITIONS
.\bin\windows\kafka-topics.bat --bootstrap-server localhost:9092 --describe topic topic_name





FOR DELETING THE RECORDS AND LOGS FROM TOPIC :::::::::::::::::::

kafka-topics --bootstrap-server localhost:9092 \
--topic my-topic \
--delete

.\bin\windows\kafka-topics.bat --create --topic topic_demo --bootstrap-server localhost:9092

myWikiTopic

.\bin\windows\kafka-topics.bat --bootstrap-server localhost:9092 --topic myWikiTopic --delete


delete.topic.enable=true

.\bin\windows\kafka-topics.bat  --delete --topic myWikiTopic --zookeeper localhost:2181

.\bin\windows\kafka-topics.bat --zookeeper localhost:2181 --list












 ### Command to produce messages in to the Kafka-Topic   ### below both of the commands for kafka-console-producer will work i.e. with only --broker-list or with only --bootstrap-server
 * .\bin\windows\kafka-console-producer.bat --broker-list  localhost:9092 --topic greetings
 *
 * .\bin\windows\kafka-console-producer.bat  --bootstrap-server localhost:9092 --topic greetings
 *
 *
 * #### Command to consume message from the Kafka-topic
 * .\bin\windows\kafka-console-consumer.bat  --bootstrap-server localhost:9092 --topic greetings-uppercase
 *
 *
 *
 *
 *
 * Publish to greetings topic with key and value
 * .\bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic greetings --property "key.separator=-" --property "parse.key=true"
 *
 * .\bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic greetings-spanish-uppercase --property "key.separator=-" --property "parse.key=true"
 *
 * Command to consume with Key
 * .\bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic greetings-uppercase --from-beginning -property "key.separator=-" --property "print.key=true"
 *
 
 
 =========================================================================================================================
 
 
 JAVA-TECHIE  APACHE KAFKA START :::::::::::::::::::::::::::::::::
 
 
 # documents

## Open Source Kafka Startup in local ##

1. Start Zookeeper Server

    ```sh bin/zookeeper-server-start.sh config/zookeeper.properties```

2. Start Kafka Server / Broker

    ```sh bin/kafka-server-start.sh config/server.properties```

3. Create topic

    ```sh bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic NewTopic --partitions 3 --replication-factor 1```

4. list out all topic names

    ``` sh bin/kafka-topics.sh --bootstrap-server localhost:9092 --list ```

5. Describe topics
  
    ``` sh bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic NewTopic ```

6. Produce message

    ```sh bin/kafka-console-producer.sh --broker-list localhost:9092 --topic NewTopic```


7. consume message

    ``` sh bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic NewTopic --from-beginning ```


## Confluent Kafka Community Edition in local ##

1. Start Zookeeper Server

    ```bin/zookeeper-server-start etc/kafka/zookeeper.properties```

2. Start Kafka Server / Broker

    ```bin/kafka-server-start etc/kafka/server.properties```

3. Create topic

    ```bin/kafka-topics --bootstrap-server localhost:9092 --create --topic NewTopic1 --partitions 3 --replication-factor 1```

4. list out all topic names

    ``` bin/kafka-topics --bootstrap-server localhost:9092 --list ```

5. Describe topics
  
    ``` bin/kafka-topics --bootstrap-server localhost:9092 --describe --topic NewTopic1 ```

6. Produce message

    ```bin/kafka-console-producer --broker-list localhost:9092 --topic NewTopic1```


7. consume message

    ```bin/kafka-console-consumer --bootstrap-server localhost:9092 --topic NewTopic1 --from-beginning ```
    
8. Send CSV File data to kafka    

   ```bin/kafka-console-producer --broker-list localhost:9092 --topic NewTopic1 <bin/customers.csv```
   
   
 
 
 
 
 


=========================================================================================================================

StreamsConfig Values::
application.id=greetings-app

warning related to ClientMetrics ::
NullPointerException inStream parameter is null



ConsumerConfig values:
you can see auto.offset.reset=latest  # just like we had configured


loook for this in the console ::
PARTITIONS_ASSIGNED to RUNNING to check that our Kafka-Stream Application is up and running and ready for the processing


ProducerConfig values:


see a message State transition from PARTITIONS_ASSIGNED to RUNNING # this means our kafka-streams application up and running and ready

01:10:39.555 [greetings-app-a8d5d556-8ebb-4af9-8f8c-842cfa42c949-StreamThread-1] INFO  o.a.k.s.p.internals.StreamThread - stream-thread [greetings-app-a8d5d556-8ebb-4af9-8f8c-842cfa42c949-StreamThread-1] State transition from PARTITIONS_ASSIGNED to RUNNING

01:10:39.555 [greetings-app-a8d5d556-8ebb-4af9-8f8c-842cfa42c949-StreamThread-1] INFO  o.apache.kafka.streams.KafkaStreams - stream-client [greetings-app-a8d5d556-8ebb-4af9-8f8c-842cfa42c949] State transition from REBALANCING to RUNNING










----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


var modifiedStream=greetingsStream
              //  .filter((key, value) -> value.length()>5)   // this will filter out values less than 5 and pass only value length greater than 5
                .filterNot((key,value) -> value.length()>5) // this will filter out values greater than 5 and pass only value length less than 5
                .mapValues((readOnlyKey, value) ->value.toUpperCase());



the map ::
this operator is used when we have to transform the key and value to another form.

var modifiedStream=greetingsStream
              //  .filter((key, value) -> value.length()>5)   // this will filter out values less than 5 and pass only value length greater than 5
                .filterNot((key,value) -> value.length()>5) // this will filter out values greater than 5 and pass only value length less than 5
                .map((key,value) -> KeyValue.pair(key.toUpperCase(),value.toUpperCase()));
				
in map() we can do all kinds of transformations to uppercase or lowercase or to integer form anything but 
important thing is when we return from the map() operator we need to return both the key and value and thats why above KeyValue.pair() function is used 






mapValues:::::
this is used when we have a usecase to transform just the values in the Kafka Stream.
just like we din in the below 

var modifiedStream=greetingsStream
              //  .filter((key, value) -> value.length()>5)   // this will filter out values less than 5 and pass only value length greater than 5
                .filterNot((key,value) -> value.length()>5) // this will filter out values greater than 5 and pass only value length less than 5
                .mapValues((readOnlyKey, value) ->value.toUpperCase());				









flatMap :::::
this operator can be used when a single event is going to create multiple possible events downstream
using map we convert each value into a key-value pair
flatMap takes care of flattening the collection here we are returning the list of key-value pairs
but flatMap takes care of flattening the collection and send them as individual events downstream

Apple ->> A P P L E


var modifiedStream=greetingsStream
             .filter((key,value) -> value.length()>5)
			 .flatMap((key,value) -> {
				var newValue=Arrays.asList(value.split(""));
				var keyValueList= newValue.stream()
											.map(v -> KeyValue.pair(key.toUpperCase(),v.toUpperCase()))
											.collect(Collectors.toList());
				return keyValueList;
				
			 });
               





flatMapValues ::::
this is very similar to flatMap but we just access and change the values.

var upperCaseStream = greetingsStream
										.filter((key,value) -> value.length()>5)
										.flatMapValues((readOnlyKey, value) -> {
										var newValue= Arrays.asList(value.split(""));
										return newValue;
										})





 var modifiedStream=greetingsStream
              //  .filter((key, value) -> value.length()>5)   // this will filter out values less than 5 and pass only value length greater than 5
                //.filterNot((key,value) -> value.length()>5) // this will filter out values greater than 5 and pass only value length less than 5
                //.mapValues((readOnlyKey, value) ->value.toUpperCase());
                 //     .map((key,value) -> KeyValue.pair(key.toUpperCase(),value.toUpperCase()));
                /*.flatMap((key,value) -> {
                    var newValue= Arrays.asList(value.split(""));
                    var keyValueList= newValue.stream()
                            .map(v -> KeyValue.pair(key.toUpperCase(),v.toUpperCase()))
                            .collect(Collectors.toList());
                    return keyValueList;
                }); */
                .flatMapValues((readOnlyKey,value) -> {
            var newValue= Arrays.asList(value.split(""));
            return newValue.stream()
                    .map("Hi"::concat)
                    .map(String::toUpperCase)
                    .toList();
        });
		
		
		
		
		
merge :::
this operator is used to combine two independent Kafka Streams from two different topics into a single Kafka Stream

private static KStream<String, Greeting> getCustomGreetingKStream(StreamsBuilder streamsBuilder) {
        //KStream<String,String> greetingsStream= streamsBuilder.stream(GREETINGS); // Here this way Stream has no clue about what the Serialization and Deserialization going to be but it still needs to get the Serialization and Deserialization and Deserialization from somewhere else so we will configure at Properties config in launcher

        // KStream<String, Greeting> greetingsStream=streamsBuilder.stream(GREETINGS, Consumed.with(Serdes.String(), GreetingSerdesFactory.greeting()));
        KStream<String, Greeting> greetingsStream=streamsBuilder.stream(GREETINGS, Consumed.with(Serdes.String(), GreetingSerdesFactory.greetingUsingGenerics()));

        // var greetingsSpanishUpperCase=streamsBuilder.stream(GREETINGS_SPANISH_UPPERCASE,Consumed.with(Serdes.String(),Serdes.String()));
        // KStream<String,Greeting> greetingsSpanishUpperCase= streamsBuilder.stream(GREETINGS_SPANISH_UPPERCASE,Consumed.with(Serdes.String(),GreetingSerdesFactory.greeting()));
        KStream<String,Greeting> greetingsSpanishUpperCase= streamsBuilder.stream(GREETINGS_SPANISH_UPPERCASE,Consumed.with(Serdes.String(),GreetingSerdesFactory.greetingUsingGenerics()));

        var mergedStream=greetingsStream.merge(greetingsSpanishUpperCase);
        return mergedStream;
    }

		

CREATING TOPOLOGY :::::::::::::


import com.mytutorial.orderskafkastreamsapp.domain.Order;
import com.mytutorial.orderskafkastreamsapp.domain.OrderType;
import com.mytutorial.orderskafkastreamsapp.domain.Revenue;
import com.mytutorial.orderskafkastreamsapp.serdes.OrderSerdesFactory;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.kstream.*;

import java.math.BigDecimal;

/**
 *Command to consume from Kafka-Topic
 * .\bin\windows\kafka-console-consumer.bat  --bootstrap-server localhost:9092 --topic orders-restaurant
 * .\bin\windows\kafka-console-consumer.bat  --bootstrap-server localhost:9092 --topic orders-general
 *
 */
@Slf4j
public class OrdersTopology {
    public static final String ORDERS = "orders";

    public static final String RESTAURANT_ORDERS= "orders-restaurant";
    public static final String GENERAL_ORDERS= "orders-general";
    public static final String STORES = "stores";

    public static Topology buildTopology(){

        Predicate<? super String,? super Order> generalPredicate= (key,order) -> order.orderType().equals(OrderType.GENERAL);
        Predicate<? super String,? super Order> restaurantPredicate= (key,order) -> order.orderType().equals(OrderType.RESTAURANT);

        StreamsBuilder streamsBuilder=new StreamsBuilder();

        KStream<String, Order> orderStream= streamsBuilder.stream(ORDERS, Consumed.with(Serdes.String(), OrderSerdesFactory.orderSerde()));

        orderStream.print(Printed.<String,Order>toSysOut().withLabel(ORDERS));

        splitUsingBranched(orderStream, generalPredicate, restaurantPredicate);

        //mySplitUsingFilter(orderStream);


        return streamsBuilder.build();
    }

    //   tutorial way of doing the split of OrderStream into Two General and Restaurant and produce to two different kafka-topics
    private static void splitUsingBranched(KStream<String, Order> orderStream, Predicate<? super String, ? super Order> generalPredicate, Predicate<? super String, ? super Order> restaurantPredicate) {

        ValueMapper<Order,Revenue> revenueValueMapper=order -> new Revenue(order.locationId(), order.finalAmount());

        // BranchedKStream<K, V> branch(Predicate<? super K, ? super V> var1, Branched<K, V> var2);
        orderStream.split(Named.as("Restaurant_General_Orders"))
                        .branch(generalPredicate,
                        Branched.withConsumer(generalOrderStream ->{

                            generalOrderStream.print(Printed.<String,Order>toSysOut().withLabel("GENERAL-ORDER-STREAM"));

                            generalOrderStream.mapValues((readOnlyKey,order) -> revenueValueMapper.apply(order))
                            .to(GENERAL_ORDERS,Produced.with(Serdes.String(),OrderSerdesFactory.revenueSerde()));

                                }))
                                .branch(restaurantPredicate,
                                        Branched.withConsumer(restaurantOrderStream ->{

                                            restaurantOrderStream.print(Printed.<String,Order>toSysOut().withLabel("RESTAURANT-ORDER-STREAM"));

                                            restaurantOrderStream
                                                    .mapValues((readOnlyKey,order) -> revenueValueMapper.apply(order))
                                                    .to(RESTAURANT_ORDERS, Produced.with(Serdes.String(),OrderSerdesFactory.revenueSerde()));
                                        })
                                        );
    }

    // my way of doing the split of OrderStream into Two General and Restaurant
    private static void mySplitUsingFilter(KStream<String, Order> orderStream) {
        KStream<String,Revenue> restaurantOrderStream= orderStream
                                .filter((k,v)->v.orderType().equals(OrderType.RESTAURANT))
                .mapValues((readOnlyKey,order) ->{
                    String locationId= order.locationId();
                    BigDecimal revenue=order.finalAmount();
                    return new Revenue(locationId,revenue);
                });

        restaurantOrderStream.print(Printed.<String,Revenue>toSysOut().withLabel("RESTAURANT-ORDER-STREAM"));


        KStream<String,Revenue> generalOrderStream= orderStream
                                .filter((k,v)->v.orderType().equals(OrderType.GENERAL))
                .mapValues((readOnlyKey,order) ->{
                    String locationId= order.locationId();
                    BigDecimal revenue=order.finalAmount();
                    return new Revenue(locationId,revenue);
                });

        generalOrderStream.print(Printed.<String,Revenue>toSysOut().withLabel("GENERAL-ORDER-STREAM"));


        restaurantOrderStream.to(RESTAURANT_ORDERS, Produced.with(Serdes.String(), OrderSerdesFactory.revenueSerde()));

        generalOrderStream.to(GENERAL_ORDERS,Produced.with(Serdes.String(),OrderSerdesFactory.revenueSerde()));
    }
}


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

GREETINGS APP::::::::::::::::::::::::::::::::::::::::




public record Greeting(String message,LocalDateTime timeStamp) {
}





import com.mytutorial.greetingstreams.domain.Greeting;
import com.mytutorial.greetingstreams.serdes.GreetingSerdes;
import com.mytutorial.greetingstreams.serdes.GreetingSerdesFactory;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.KeyValue;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.kstream.Consumed;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.Printed;
import org.apache.kafka.streams.kstream.Produced;

import java.util.Arrays;
import java.util.stream.Collectors;

/**
 * Topology is a class in Kafka Streams that which basically holds the whole flow of the Kafka Streams
 * this holds the whole processing logic for our Kafka Streams Application
 *
 * serdes is a factory class in kafka streams that takes care of serialization and deserialization of key and value this is little different from Kafka Consumer and Producer Api
 * because when we are building Kafka Consumer and Producer API  we specifically call out the serializer and deserializer properties and provide the appropriate class for serialization and deserialization
 *
 * .\bin\windows\kafka-console-consumer.bat  --bootstrap-server localhost:9092 --topic greetings-uppercase
 */
@Slf4j
public class GreetingsTopology {

    // Source topic name
    public static String GREETINGS="greetings";

    // Destination topic name
    public static String GREETINGS_UPPERCASE="greetings-uppercase";
    public static String GREETINGS_SPANISH_UPPERCASE="greetings-spanish-uppercase";

    public static Topology buildTopology(){

        // using StreamBuilder as building block we can define Source-Processor and StreamProcessing Logic and Sink-Processor
        // used StreamBuilder to build a pipeline to read Data from the Kafka-Topic then Performed Data-Enrichment(here modifying from lowercase to uppercase) then writing the data back to the Kafka-Topic
        StreamsBuilder streamsBuilder=new StreamsBuilder();

        // using java feature type inference and assigning the value to the variable
        // consuming the msg from GREETINGS topic
        // this uses the consumer api
        // var greetingsStream=streamsBuilder.stream(GREETINGS, Consumed.with(Serdes.String(),Serdes.String())); // when used with Consumed.with then first Serdes.String() key deserializer and second Serdes.String() is value deserializer
        // var mergedStream = getStringGreetingKStream(streamsBuilder);
        var mergedStream = getCustomGreetingKStream(streamsBuilder);

        // anytime the message is read it's going to print to the console with greetingsStream :
        // this way we can analyze or look what is going on once we publish the message into Kafka-topic  and how are KafkaStreams is executing this topology
        // mergedStream.print(Printed.<String,String>toSysOut().withLabel("mergedStream"));
        mergedStream.print(Printed.<String,Greeting>toSysOut().withLabel("mergedStream"));

        // using greetinsStream we can build our Processing logic : we are going to convert the value from lowercase to uppercase
        var modifiedStream=mergedStream
                //.filter((key, value) -> value.length()>5)   // this will filter out values less than 5 and pass only value length greater than 5
                //.filterNot((key,value) -> value.length()>5) // this will filter out values greater than 5 and pass only value length less than 5
                //.peek((key,value) -> log.info("after filter Key:{}, Value:{}",key,value))   // any changes made in peek() will not be retained or reflected with stream pipeline
                .mapValues((readOnlyKey, value) ->{
                    return new Greeting(value.message().toUpperCase(),value.timeStamp());
                });
                 //     .map((key,value) -> KeyValue.pair(key.toUpperCase(),value.toUpperCase()));
                /*.flatMap((key,value) -> {
                    var newValue= Arrays.asList(value.split(""));
                    var keyValueList= newValue.stream()
                            .map(v -> KeyValue.pair(key.toUpperCase(),v.toUpperCase()))
                            .collect(Collectors.toList());
                    return keyValueList;
                }); */
                /*.flatMapValues((readOnlyKey,value) -> {
            var newValue= Arrays.asList(value.split(""));
            return newValue.stream()
                    .map("Hi"::concat)
                    .map(String::toUpperCase)
                    .toList();
        });*/

        // anytime the message is modified it's going to print to the console with modifiedStream : this way we can look analyze how KafkaStreams is executing this topology
        modifiedStream.print(Printed.<String,Greeting>toSysOut().withLabel("modifiedStream"));

        // writing the message to GREETINGS_UPPERCASE topic
        // this uses the producer api
        // modifiedStream.to(GREETINGS_UPPERCASE, Produced.with(Serdes.String(),Serdes.String())); // when used with Produced.with then first Serdes.String() key serializer and second Serdes.String() is value serializer
        // modifiedStream.to(GREETINGS_UPPERCASE); // here we removed Produced.with Serdes.String() because we have configured DEFAULT_KEY_SERDE_CLASS_CONFIG and DEFAULT_VALUE_SERDE_CLASS_CONFIG at Properties config with Serdes.StringSerde.class in launcher from there we Streams get the key and value serializer and deserializer
        // modifiedStream.to(GREETINGS_UPPERCASE, Produced.with(Serdes.String(),GreetingSerdesFactory.greeting()));
        modifiedStream.to(GREETINGS_UPPERCASE, Produced.with(Serdes.String(),GreetingSerdesFactory.greetingUsingGenerics()));

        return streamsBuilder.build();
    }

    private static KStream<String, String> getStringGreetingKStream(StreamsBuilder streamsBuilder) {
        KStream<String,String> greetingsStream= streamsBuilder.stream(GREETINGS); // Here this way Stream has no clue about what the Serialization and Deserialization going to be but it still needs to get the Serialization and Deserialization and Deserialization from somewhere else so we will configure at Properties config in launcher

        // var greetingsSpanishUpperCase=streamsBuilder.stream(GREETINGS_SPANISH_UPPERCASE,Consumed.with(Serdes.String(),Serdes.String()));
        KStream<String,String> greetingsSpanishUpperCase= streamsBuilder.stream(GREETINGS_SPANISH_UPPERCASE);

        var mergedStream=greetingsStream.merge(greetingsSpanishUpperCase);
        return mergedStream;
    }

    private static KStream<String, Greeting> getCustomGreetingKStream(StreamsBuilder streamsBuilder) {
        //KStream<String,String> greetingsStream= streamsBuilder.stream(GREETINGS); // Here this way Stream has no clue about what the Serialization and Deserialization going to be but it still needs to get the Serialization and Deserialization and Deserialization from somewhere else so we will configure at Properties config in launcher

        // KStream<String, Greeting> greetingsStream=streamsBuilder.stream(GREETINGS, Consumed.with(Serdes.String(), GreetingSerdesFactory.greeting()));
        KStream<String, Greeting> greetingsStream=streamsBuilder.stream(GREETINGS, Consumed.with(Serdes.String(), GreetingSerdesFactory.greetingUsingGenerics()));

        // var greetingsSpanishUpperCase=streamsBuilder.stream(GREETINGS_SPANISH_UPPERCASE,Consumed.with(Serdes.String(),Serdes.String()));
        // KStream<String,Greeting> greetingsSpanishUpperCase= streamsBuilder.stream(GREETINGS_SPANISH_UPPERCASE,Consumed.with(Serdes.String(),GreetingSerdesFactory.greeting()));
        KStream<String,Greeting> greetingsSpanishUpperCase= streamsBuilder.stream(GREETINGS_SPANISH_UPPERCASE,Consumed.with(Serdes.String(),GreetingSerdesFactory.greetingUsingGenerics()));

        var mergedStream=greetingsStream.merge(greetingsSpanishUpperCase);
        return mergedStream;
    }
}














import com.mytutorial.greetingstreams.topology.GreetingsTopology;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.admin.AdminClient;
import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsConfig;

import java.util.List;
import java.util.Properties;
import java.util.stream.Collectors;

@Slf4j
public class GreetingsStreamApp {

    public static void main(String[] args) {

        // APPLICATION_ID_CONFIG is the Identifier of this application we can think of as Consumer-Group-IDs in Kafka-consumer
        // APPLICATION_ID_CONFIG is important this is how KafkaSteams Api is going to maintain its bookmark which means this is
        // how it knows what was the message that read in the Kafka-topic so when we restart the application it knows where to start read from the kafka-topic
        Properties properties=new Properties();
        properties.put(StreamsConfig.APPLICATION_ID_CONFIG,"greetings-app");
        properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9092");
        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"latest");

        // Another way of configuring Kafka Serdes Key and Value Serializer and Deserializer if we don't specifically specify at topology with Consumed.with and Produced.with
        // we can use this kind of configuration only when we are sure about that we are going to deal with only one particular key and value serializer and deserializer i.e. Default key and value Serde class
        //properties.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class);
        //properties.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class);

        createTopics(properties,List.of(GreetingsTopology.GREETINGS,GreetingsTopology.GREETINGS_UPPERCASE,GreetingsTopology.GREETINGS_SPANISH_UPPERCASE));

        var greetingsTopology= GreetingsTopology.buildTopology();

        // KafkaStreams this is the class which takes care of starting our application basically this is the one going to execute our topology
        // this is the one actually going to execute our topology the pipeline which involves Source-Processor and Stream-Processing-Logic and Sink-Processor
        // KafkaStreams instance start-up the application and properties tells which kafka cluster its going to interact with
        var kafkaStreams=new KafkaStreams(greetingsTopology, properties);

        // when this application shuts-down we need to make sure that releasing all the resources that are accessed by this KafkaStreams Application
        // so we can do a Shutdown-Hook anytime we shutdown this app this shutdown-hook is going to be invoked and it's going to call the close function of KafkaStreams
        //  this will do the graceful shutdown this will take care of releasing all the resources that was accessed by this Greetings-Application
        Runtime.getRuntime().addShutdownHook(new Thread(kafkaStreams::close));

        try{
            kafkaStreams.start();
        }catch (Exception e){
            log.error("Exception in the Starting the Stream : {}",e.getMessage(),e);
        }

    }

    // this function is going to create kafka-topic for us.
    // so using two Kafka-Topics 1 GREETINGS and 2 GREETINGS_UPPERCASE
    // here this is a programmatic way of creating Kafka-topics passing the topics-name as list and configs represents the broker details for this application
    private static void createTopics(Properties config, List<String> greetings) {

        AdminClient admin = AdminClient.create(config);
        var partitions = 2;
        short replication  = 1;

        // creating instance of NewTopic
        var newTopics = greetings
                .stream()
                .map(topic -> {
                    return new NewTopic(topic, partitions, replication);
                })
                .collect(Collectors.toList());

        var createTopicResult = admin.createTopics(newTopics);
        try {
            createTopicResult
                    .all().get();
            log.info("topics are created successfully");
        } catch (Exception e) {
            log.error("Exception creating topics : {} ",e.getMessage(), e);
        }
    }
}


















import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.SerializationFeature;
import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;
import com.mytutorial.greetingstreams.exception.GreetingException;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.common.serialization.Serializer;

/**
 * Creating a Generic Serializer in order to scale up the project if in future there is need
 * because using Generic Serializer we can scale up easily we don't need to create specific type of Serializer and Deserializer
 * like we did create GreetingSerializer and GreetingDeserializer because in future another type came then we have to create
 * again serializer and deserializer which is not recommended way .... we need Generic Serializer to reuse
 *
 * Same goes for Generic Deserializer
 * @param <T>
 */
@Slf4j
public class JSONSerializer<T> implements Serializer<T> {

    private final ObjectMapper objectMapper=new ObjectMapper()
            .registerModule(new JavaTimeModule())
            .configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS,false);

    @Override
    public byte[] serialize(String topic, T data) {
        try {
            return objectMapper.writeValueAsBytes(data);
        } catch (JsonProcessingException e) {
            log.error("JsonProcessingException JSONSerializer:: serialize :: {}",e.getMessage(),e);
            throw new GreetingException(e);
        } catch (Exception e){
            log.error("Exception JSONSerializer:: serialize :: {}",e.getMessage(),e);
            throw new GreetingException(e);
        }
    }
}















import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.SerializationFeature;
import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;
import com.mytutorial.greetingstreams.exception.GreetingException;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.common.serialization.Deserializer;

import java.io.IOException;

@Slf4j
public class JSONDeserializer<T> implements Deserializer<T> {

    private Class<T> destinationClass;
    private final ObjectMapper objectMapper=new ObjectMapper()
            .registerModule(new JavaTimeModule())
            .configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS,false);

    public JSONDeserializer(Class<T> destinationClass) {
        this.destinationClass = destinationClass;
    }

    @Override
    public T deserialize(String topic, byte[] data) {
        if(data==null) return null; // if data is null i dont want to execute our deserialization logic
        try {
            return objectMapper.readValue(data,destinationClass);
        } catch (IOException e) {
            log.error("IOException JSONDeserializer:: deserialize :: {}",e.getMessage(),e);
            throw new GreetingException(e);
        }catch (Exception e){
            log.error("Exception JSONDeserializer:: deserialize :: {}",e.getMessage(),e);
            throw new GreetingException(e);
        }
    }
}











import com.mytutorial.greetingstreams.domain.Greeting;
import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serdes;

/**
 * this GreetingSerdesFactory class is created to use it with Consumed.with() and Produced.with()
 * there in that in key section its String so Serdes.String() is used but in value section it is Greeting in JSON format
 * so we created the GreetingSerdesFactory which has greeting() method which returns GreetingSerdes which is type of Serde<Greeting>
 *
 *     just like Serdes.String() method is used in Consumed.with() and Produced.with()
 *
 *     in Serdes.class
 *     public static Serde<String> String() {
 *         return new StringSerde();
 *     }
 *
 *     public static final class StringSerde extends WrapperSerde<String> {
 *         public StringSerde() {
 *             super(new StringSerializer(), new StringDeserializer());
 *         }
 *     }
 *
 *     public static class WrapperSerde<T> implements Serde<T> {} this WrapperSerde implements Serde of type that we specify as Target
 *
 */
public class GreetingSerdesFactory {

    public static Serde<Greeting> greeting(){
        return new GreetingSerdes();
    }

    // in future if we want a different type we basically create another factory function then we change the type over here
    // like here we used Greeting we can use any other type accordingly
    public static Serde<Greeting> greetingUsingGenerics(){
        JSONSerializer<Greeting> serializer = new JSONSerializer<>();
        JSONDeserializer<Greeting> deserializer=new JSONDeserializer<>(Greeting.class);
        return Serdes.serdeFrom(serializer, deserializer);
    }
}













import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.serialization.StringSerializer;

import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.ExecutionException;

@Slf4j
public class ProducerUtil {


    static KafkaProducer<String, String> producer = new KafkaProducer<String, String>(producerProps());

    public static Map<String, Object> producerProps(){

        Map<String,Object> propsMap = new HashMap<>();
        propsMap.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        propsMap.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        propsMap.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        return propsMap;

    }


    public static RecordMetadata publishMessageSync(String topicName, String key, String message ){

        ProducerRecord<String,String> producerRecord = new ProducerRecord<>(topicName, key, message);
        RecordMetadata recordMetadata=null;

        try {
            log.info("producerRecord : " + producerRecord);
            recordMetadata = producer.send(producerRecord).get();
        } catch (InterruptedException e) {
            log.error("InterruptedException in  publishMessageSync : {}  ", e.getMessage(), e);
        } catch (ExecutionException e) {
            log.error("ExecutionException in  publishMessageSync : {}  ", e.getMessage(), e);
        }catch(Exception e){
            log.error("Exception in  publishMessageSync : {}  ", e.getMessage(), e);
        }
        return recordMetadata;
    }
}









import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.SerializationFeature;
import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;

import com.mytutorial.greetingstreams.domain.Greeting;
import lombok.extern.slf4j.Slf4j;

import java.time.LocalDateTime;
import java.util.List;

import static com.mytutorial.greetingstreams.producer.ProducerUtil.publishMessageSync;


@Slf4j
public class GreetingMockDataProducer {

    static String GREETINGS = "greetings";

    public static void main(String[] args) {
        ObjectMapper objectMapper = new ObjectMapper()
                .registerModule(new JavaTimeModule())
                .configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false);

        englishGreetings(objectMapper);
        spanishGreetings(objectMapper);

    }

    private static void spanishGreetings(ObjectMapper objectMapper) {
        var spanishGreetings = List.of(
                new Greeting("¡Hola buenos dias!", LocalDateTime.now()),
                new Greeting("¡Hola buenas tardes!", LocalDateTime.now()),
                new Greeting("¡Hola, buenas noches!", LocalDateTime.now())
        );
        spanishGreetings
                .forEach(greeting -> {
                    try {
                        var greetingJSON = objectMapper.writeValueAsString(greeting);
                        var recordMetaData = publishMessageSync(GREETINGS, null, greetingJSON);
                        log.info("Published the alphabet message : {} ", recordMetaData);
                    } catch (JsonProcessingException e) {
                        throw new RuntimeException(e);
                    }
                });
    }

    private static void englishGreetings(ObjectMapper objectMapper) {
        var englishGreetings = List.of(
                new Greeting("Hello, Good Morning!", LocalDateTime.now()),
                new Greeting("Hello, Good Evening!", LocalDateTime.now()),
                new Greeting("Hello, Good Night!", LocalDateTime.now())
        );

        englishGreetings
                .forEach(greeting -> {
                    try {
                        var greetingJSON = objectMapper.writeValueAsString(greeting);
                        var recordMetaData = publishMessageSync(GREETINGS, null, greetingJSON);
                        log.info("Published the alphabet message : {} ", recordMetaData);
                    } catch (JsonProcessingException e) {
                        throw new RuntimeException(e);
                    }
                });
    }

}





=========================================================================================================================

TOPOLOGY , STREAMS, TASKS under the hood::::::::::::::::


Kafka Streams - Topology, Task & Threads

1. Topology in Kafka Streams means its a data pipeline or template on how the data is going to be flow or handled in our Kafka Streams Application

2. A Topology in itself does not execute on its own.


when we invoke start() funciton on KafkaStreams i.e. kafkaStreams.start(); then  there are two things happens behind the scenes  ::
 1. Create "Tasks" for the topology
 2. Execute "Tasks" by Stream Threads
 
 Tasks in Kafka Streams ::
 A Task is basically the unit of work in a Kafka Streams application 
 How many Tasks can be present in a Kafka Streams Application ? 
	The maximum number of tasks is determined by the number of partitions of the source topic.
	for example :: if the source topic has four partitions then we will have four tasks created behind the scenes for the Kafka Streams Application.
    The benefit of creating tasks is that we can execute tasks in parallel by the Kafka Streams Application

	Kakfa Streams splits data into partitions inside the topic and each and every partition is independent of one another so when you have four tasks created in this example you can 
	parallely process them to speed up the overall process. 
	But the Parallelism in Kafka Streams Application is determined by Stream-Threads or we can also create multiple instances of Kafka-Stream Application.
	
	Threads or Stream Threads are the ones that execute the Tasks in a Kafka Streams application.
	By default the number of stream threads is 1.
		The value of threads is controlled by the num.stream.threads. property
		you can modify it based on how many tasks you want to execute in parallel.
		
		properties.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG,"2");
		
		
Parallelism in KAFKA Streams::::

approach 1 :

if our KAFKA-topic has 4 partitions and our kafka stream application is streaming the topic and we have 4 tasks		then by default only one streams-thread is going to execute each tasks.
but if we increase the streams-thread by num.stream.threads property and seting up value to 4 i.e. 4 threads then each thread will execute the task and this is parallelism
but if num.stream.threads property and seting up value to 2 then 4 tasks will be evenly executed by two available threads. and this way we achieve parallelism

approach 2 :

if our KAFKA-topic has 4 partitions and our kafka stream application is streaming the topic and we have 4 tasks and if our kafka-streams applications multiple instances are running 
without setting up num.stream.threads property that means single stream-thread is running and executing tasks and this way we can achieve parallelism but each and every instance share the same application-id . in such setup tasks will be distributed to running multiple instances 


Whats the Ideal number of Stream Threads ?
Keep the Stream threads size equal to the number of cores in the machine .
		
		Runtime.getRuntime().availableProcessors();
		


KAFKA Streams & Consumer Groups 
Kafka Streams behind the scenes uses consumer api to stream from a topic.
Kafka Streams behind the scenes uses producer api to produce data to Kafka-Topic

application.id given as part of our Kafka-Stream Application is equivalent to the group.id in Kafka Consumer 
application.id is equivalent to group.id in KAFKA Consumer 

Kakfa Streams gets all the benefits of consumer groups.

Tasks are split across multiple instances using the consumer group concept.


Kafka Streams API also gets the Fault Tolerance behavior from the Consumer Groups Concept
Fault Tolerance : if one instance goes down then rebalance will be triggered to redistribute the tasks.


StreamsConfig value or property  to set number of Stream-Threads :::: num.stream.threads
		
Search for New active tasks : [0_1,0_0]  # here two task is created since two partitions are set but still one Stream Thread i.e. StreamThread-1
	
    



// setting up the number of StreamThreads manually
        // Runtime.getRuntime().availableProcessors(); // we can get the number of threads to set for number os StreamThreads.
        config.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG,"2");
		
		observe two StreamThreads i.e. StreamThread-1, StreamThread-2 and two tasks and equally executed by two Stream Threads.

IN CONSOLE ::::::::::::

19:02:00.553 [main] INFO  o.apache.kafka.streams.StreamsConfig - StreamsConfig values: 
num.stream.threads = 2


19:02:00.781 [orders-app-f7c95023-c761-43e6-9090-efea1a5301f0-StreamThread-1] INFO  o.a.k.s.p.internals.TaskManager - stream-thread [orders-app-f7c95023-c761-43e6-9090-efea1a5301f0-StreamThread-1] Handle new assignment with:
	New active tasks: [0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: []
19:02:00.781 [orders-app-f7c95023-c761-43e6-9090-efea1a5301f0-StreamThread-2] INFO  o.a.k.s.p.internals.TaskManager - stream-thread [orders-app-f7c95023-c761-43e6-9090-efea1a5301f0-StreamThread-2] Handle new assignment with:
	New active tasks: []
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: []
19



========================================================================================================================

ERROR EXCEPTION HANDLING IN KAFKA STREAMS:::::::::::::


A typical Kafka Streams Application has three components ::
1. Deserialization
2. TOpology where our business logic resides
3. Serialization


if we have sink processor we uses serialization process to write the data into the output Kafka-Topic
this means that there are three places failures can happen

1. Deserialization or Transient Errors at the entry Level. (transient error means temporary issue with bcoz of network connection or partition rebalance)

2. RuntimeException in the Application Code (Topology) 
3. Serialization or Transient Errors when producing the data.


ErrorHandlers in KAFKA Streams :::
 for custom logic we can build our class implementing this ErrorHandler interfaces and wire that into our application
ERROR ::::														Error Handler
Deserialization													DeserializationExceptionHandler interface
Application Error (Topology)								StreamsUncaughtExceptionHandler interface
Serialization														ProductionExceptionHandler interface 


---------------------------------------------------------------------------------------------------------------------------------------------------

ERRORHANDLER AT DESERIALIZATION ::::::::::::::::::::::::::::::::::::::::::::


properties.put(StreamsConfig.DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG,LogAndContinueExceptionHandler.class);

LogAndContinueExceptionHandler.class implements DeserializationExceptionHandler
if this one is default set then application will continue even at failures


LogAndFailExceptionHandler implements DeserializationExceptionHandler  if this one is default set then application will fail and stop at failures



creating 3. Custom Deserialization Error Handler


import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.streams.errors.DeserializationExceptionHandler;
import org.apache.kafka.streams.processor.ProcessorContext;

import java.util.Map;

/**
 *  adding a custom DeserializationExceptionHandler class here we are also adding a threshold error counter which
 *  also checks that each instance can handle upto 2 errors only after that application will fail and stop or exit.
 *
 *  since for this GreetingStream Application we have configured or set the StreamThread to 2 by below given property configuration in GreetingStreamApp (a launcher class)
 *  properties.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG,"2");
 *  so that means there will be 2 StreamThread instances per application and each StreamThread instance handles the tasks
 *  and when each StreamThread instance reaches the threshold value of errorCounter upto 2 then the application will fail and exit
 *
 *   // setting up the number of StreamThreads manually
 *         // Runtime.getRuntime().availableProcessors(); // we can get the number of threads to set for number os StreamThreads.
 *         properties.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG,"2");
 *          
 *          Also configuring and setting up the Custom DeserializationExceptionHandler for our Kafka-Stream Application 
 *         properties.put(StreamsConfig.DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG, GreetingExceptionDeserializationExceptionHandler.class);
 */
@Slf4j
public class GreetingExceptionDeserializationExceptionHandler implements DeserializationExceptionHandler {

    // adding errorCounter Threshold value i.e. how many error can handle in this instance
    int errorCounter=0;
    @Override
    public DeserializationHandlerResponse handle(ProcessorContext processorContext, ConsumerRecord<byte[], byte[]> consumerRecord, Exception e) {
        log.error(" GreetingExceptionDeserializationExceptionHandler : {} , and Kafka Record is : {}",e.getMessage(),consumerRecord,e);
        log.info("ErrorCounter : {}",errorCounter);
        if(errorCounter<2){
            errorCounter++;
            return DeserializationHandlerResponse.CONTINUE;
        }
        return DeserializationHandlerResponse.FAIL;
    }

    @Override
    public void configure(Map<String, ?> configs) {

    }
}

Observe and Analyse
CHECK ERROR COUNTER IN CONSOLE MESSAGE 
CHECK EXCEPTION OCCURED IN CONSOLE MESSAGE

look for these in console one by one ::::  GreetingExceptionDeserializationExceptionHandler :	
											                  Kafka Record is : 
															  ErrorCounter : 

OBSERVE THE ErrorCounter happened in StreamThread-1 when i entered "abc" in kafka producer console	 after 2 times  on Third time appliction exited.
										 

OUTPUT :::::::::::::::::::::::::::::::::::


12:16:56.883 [greetings-app-3f0431e2-2942-4336-9d2e-c33bd29444fc-StreamThread-1] ERROR c.m.g.e.GreetingExceptionDeserializationExceptionHandler -  GreetingExceptionDeserializationExceptionHandler : com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'abc': was expecting (JSON String, Number, Array, Object or token 'null', 'true' or 'false')
 at [Source: (byte[])"abc"; line: 1, column: 4] , and Kafka Record is : ConsumerRecord(topic = greetings, partition = 0, leaderEpoch = 0, offset = 36, CreateTime = 1709275615829, serialized key size = -1, serialized value size = 3, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = [B@4c9173b2)
com.mytutorial.greetingstreams.exception.GreetingException: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'abc': was expecting (JSON String, Number, Array, Object or token 'null', 'true' or 'false')
 at [Source: (byte[])"abc"; line: 1, column: 4]
 
 

12:16:56.883 [greetings-app-3f0431e2-2942-4336-9d2e-c33bd29444fc-StreamThread-1] INFO  c.m.g.e.GreetingExceptionDeserializationExceptionHandler - ErrorCounter : 0
12:29:51.774 [greetings-app-3f0431e2-2942-4336-9d2e-c33bd29444fc-StreamThread-1] INFO  c.m.g.e.GreetingExceptionDeserializationExceptionHandler - ErrorCounter : 1
12:31:06.671 [greetings-app-3f0431e2-2942-4336-9d2e-c33bd29444fc-StreamThread-1] INFO  c.m.g.e.GreetingExceptionDeserializationExceptionHandler - ErrorCounter : 2





----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
ERRORHANDLER AT TOPOLOGY :::::::::::

4. Default & Custom Processor Error Handler:::::::::::::::::::::::::::::::::::::::::::::


public interface StreamsUncaughtExceptionHandler {
    StreamThreadExceptionResponse handle(Throwable var1);

    public static enum StreamThreadExceptionResponse {
        REPLACE_THREAD(0, "REPLACE_THREAD"),
        SHUTDOWN_CLIENT(1, "SHUTDOWN_KAFKA_STREAMS_CLIENT"),
        SHUTDOWN_APPLICATION(2, "SHUTDOWN_KAFKA_STREAMS_APPLICATION");

        public final String name;
        public final int id;

        private StreamThreadExceptionResponse(int id, String name) {
            this.id = id;
            this.name = name;
        }
    }
}

 REPLACE_THREAD(0, "REPLACE_THREAD") :::::::::::
 This is going to do provide a new thread for the error and this is going to constantly read write that failed message until that becomes recovered.

 
 
  SHUTDOWN_CLIENT(1, "SHUTDOWN_KAFKA_STREAMS_CLIENT")
  this is going to shutdown the particular thread thats causing the problem 


  SHUTDOWN_APPLICATION(2, "SHUTDOWN_KAFKA_STREAMS_APPLICATION");
  this is going to shutdown the whole application lets say we have two tasks both the tasks will be 
  shutdown.


StreamProcessorCustomErrorHandler are added differently than DeserializationExceptionHandler.

we need to add this StreamProcessorCustomErrorHandler into topology itself. so we place the instance of the StreamProcessorCustomErrorHandler into the KafkaStreams instance itself
	kafkaStream.setUncaughtExceptionHandler();

REPLACE_THREAD(0, "REPLACE_THREAD")
this is going to replace the new thread and  this is going to retry the same error again and again  and this process is going to continue indefintely 
so we do this only if we are sure that exception or error that we are getting are Transient Error and its going to go away after a bit of time 

and if that's not the case in those kind of scenarios shutdown the client  by this ::::: SHUTDOWN_CLIENT(1, "SHUTDOWN_KAFKA_STREAMS_CLIENT")
so this will make sure that error is not going to retried  but we are shuting down the client in this case 

for any other exception we are going to shutdown the application :::: SHUTDOWN_APPLICATION(2, "SHUTDOWN_KAFKA_STREAMS_APPLICATION"); thus whole application will be brought down eventually.


if we have this task run in multiple instances of the same application this wouldn't have happened the other instances should have still up and processing records from it.
without try-catch around the Transient Error in topology and when StreamProcessorCustomErrorHandler method returns StreamThreadExceptionResponse.SHUTDOWN_APPLICATION;
then only that instance will be shutdown and other instances will be up and running;


@Slf4j
public class StreamProcessorCustomErrorHandler implements StreamsUncaughtExceptionHandler {
    @Override
    public StreamThreadExceptionResponse handle(Throwable throwable) {

        log.error("StreamProcessorCustomErrorHandler Exception: {}" ,throwable.getMessage(),throwable);

        if( throwable instanceof StreamsException){
            var cause= throwable.getCause();
            if( cause.getMessage().equals("Transient Error")){

                // this is going to replace the new thread and  this is going to retry the same error again and again  and this process is going to continue indefintely
                //  so we do this only if we are sure that exception or error that we are getting are Transient Error and its going to go away after a bit of time
                // return StreamThreadExceptionResponse.REPLACE_THREAD;

                // this will make sure that error is not going to retried  but we are shuting down the client in this case
                 return StreamThreadExceptionResponse.SHUTDOWN_CLIENT;
            }
        }
        //for any other exception we are going to shutdown the application
         return StreamThreadExceptionResponse.SHUTDOWN_APPLICATION;
    }
}









the otherway of handling this is to have error handling directly in the topology itself (in my case i applied try-catch around IllegalStateException() where "Transient Error" is thrown)



this way you application is still up and running in this way we dont have to deal with this  StreamProcessorCustomErrorHandler itself 
do have this logic if you want to shutdown the application because if we dont bring the application down it may lead to some kind of corrupt data in your application and in those kind of scenarios
shuting down the applicaiton is the best option. but the alternative is if you wana ignore any error the better approach is have the error handler in the topology itself






@Slf4j
public class StreamErrorHandling {

// this is for learning purpose only
 static boolean throwErrorNow= true;
 
 public static class GreetingStreamsDeserializationExceptionHandler implements DeserializationExceptionHandler {

    // adding errorCounter Threshold value i.e. how many error can handle in this instance
    int errorCounter=0;
    @Override
    public DeserializationHandlerResponse handle(ProcessorContext processorContext, ConsumerRecord<byte[], byte[]> consumerRecord, Exception e) {
        log.error(" GreetingStreamsDeserializationExceptionHandler : {} , and Kafka Record is : {}",e.getMessage(),consumerRecord,e);
        log.info("ErrorCounter : {}",errorCounter);
        if(errorCounter++ <25){
            // errorCounter++;
            return DeserializationHandlerResponse.CONTINUE;
        }
        return DeserializationHandlerResponse.FAIL;
    }

    @Override
    public void configure(Map<String, ?> configs) {

    }
 
 
 public static class StreamRecordProducerErrorHandler implements ProductionExceptionHandler{
	
	@Override
	public ProductionExceptionHandlerResponse handle(ProducerRecord<byte[],byte[]> record, Exception exception){
			if(exception instanceof RecordTooLargeException){
					return ProductionExceptionHandlerResponse.CONTINUE;
			}
			
		return ProductionExceptionHandlerResponse.FAIL;	
	}
	
	@Override
	public void configure(Map<String,?> configs){ }
	
}

public static class StreamProcessorCustomErrorHandler implements StreamsUncaughtExceptionHandler {
    @Override
    public StreamThreadExceptionResponse handle(Throwable throwable) {
        log.error("StreamProcessorCustomErrorHandler Exception: {}" ,throwable.getMessage(),throwable);
		
        if( throwable instanceof StreamsException){
            Throwable originalException= throwable.getCause();
            if( originalException.getMessage().equals("Retryable Transient Error")){
                //return StreamThreadExceptionResponse.REPLACE_THREAD;
                return StreamThreadExceptionResponse.SHUTDOWN_CLIENT;
            }
        }
        return StreamThreadExceptionResponse.SHUTDOWN_APPLICATION;
    }
}
 
 
 public static void main(String[] args) throws IOException{
 
	final Properties streamsProps = StreamsUtils.loadProperties();
	streamsProps.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-error-handling");
	streamsProps.put(StreamsConfig.DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG, GreetingStreamsDeserializationExceptionHandler.class);
	
	streamsProps.put(StreamsConfig.DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG, StreamRecordProducerErrorHandler.class);
	
	
	
	StreamsBuilder builder=new StreamBuilder();
	final String inputTopic = streamsProps.getProperty("error.input.topic");
	final String outputTopic = streamsProps.getProperty("error.output.topic");
	
	final String orderNumberStart = "orderNumber-";
	
	KStream<String,String> streamWithErrorHandling = builder.stream( inputTopic, Consumed.with(Serdes.String(), Serdes.String()))
		.peek((key, value) -> System.out.println("Incoming record - key " +key +" value " + value));
		
		streamWithErrorHandling.filter((key, value) -> value.contains(orderNumberStart))
									.mapValues(value -> {
										if(throwErrorNow) {
											throwErrorNow = false;
											throw new IllegalStateException("Retryable transient error");
										}
										return value.substring(value.indexOf("-")+1);
									})
									.filter((key,value) -> Long.parseLong(value) > 1000)
									.peek((key, value) -> System.out.println("Outgoing record - key " + key + " value " + value))
									.to( outputTopic, Produced.with(Serdes.String(), Serdes.String()));
	
		 KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), streamsProps);
		 kafkaStreams.setUncaughtExceptionHandler(new StreamProcessorCustomErrorHandler());
		 TopicLoader.runProducer();
		
		}

}




--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

ErrorHandler at Serialization

5. Custom Production Error Handler:::::::::::::::::::::::::::::::::::::::::::::::::::::::::




/**
 * ErrorHandlers in KAFKA Streams :::
 *  for custom logic we can build our class implementing this ErrorHandler interfaces and wire that into our application
 * ERROR ::::														Error Handler
 * Deserialization													DeserializationExceptionHandler interface
 * Application Error (Topology)								StreamsUncaughtExceptionHandler interface
 * Serialization														ProductionExceptionHandler interface
 *
 * Under what scenario this ProductionExceptionHandler will come into play or under what scenario we may run into SerializationException
 * lets say we have the transform record in our stream application which is too big to be published in those kind of scenario
 * we can run into serializationException. or it could be a transient error while publishing the record into Kafka-Topic
 * meaning our kafka cluster is temporarily down or it could be rebalancing its happening so that our application is not able to
 * communicate to kafka-Cluster in those scenarios we may run into Serialization Exception.
 *
 */

@Slf4j
public class StreamsSerializationExceptionHandler implements ProductionExceptionHandler {
    @Override
    public ProductionExceptionHandlerResponse handle(ProducerRecord<byte[], byte[]> producerRecord, Exception e) {
        log.error("StreamsSerializationExceptionHandler Exception {} and ProducerRecord {} ",e.getMessage(),producerRecord,e);
        if ( e instanceof RecordTooLargeException){
            //return ProductionExceptionHandlerResponse.FAIL;
            return ProductionExceptionHandlerResponse.CONTINUE;
        }
        return ProductionExceptionHandlerResponse.CONTINUE; // this means that we don't care about any problems in publishing the records but we still want to constantly publish new records and execute by topology
    }

    @Override
    public void configure(Map<String, ?> configs) {

    }
}




if Kafka Cluster goes down and our KAFKA Stream Application is up and running then it retries to connect again and again
our KAFKA Stream Application consumer and producer shows dissconnected but still try to reconnect

in AdminClientConfig  there is retries value i.e. highest integer value 2147483647
in ProducerConfig value there is retries value i.e. highest integer value 2147483647


================================================================================================================================================================================================================================================


10 :: KTable & Global KTable

1. Introduction to KTable API  ::::::::::::::::::::

KTable is an abstraction in Kafka Streams which holds latest value for a given key.

KTable is also known as update-stream or a change log.

KTable represents the latest value for given key in a kafka Record.

Record that comes with the same key updates the previous value 

Any Record without the Key is ignored

Analogy (Relational DB)
You can think of this as an update operation in the table for a given primary-key 


How to create a KTable ?

public static Topology build(){

		StreamsBuilder streamsBuilder= new StreamsBuilder();
		
		KTable<String,String> wordsTable=streamsBuilder
																	.table("words", Consumed.with(Serdes.String(), Serdes.String()), Materialized.as("words-store")); // storeName
																	
																	
}


Materialized.as("words-store") creates state store for us 

StateStore is necessary for storing the data in an additional entity.

Materialized.as("words-store") creates a stateStore 
This is needed to retaining the data in an app crash or restart or redeployment.

The default state store is RocksDB

RocksDB is a high performance embedded database for key-value data.

This is an open source DB under the Apache 2.0

Data in the embedded key-value store is also persisted in a file system.

Data in the RocksDB also maintained in a changelog topic for Fault Tolerance.

When to use KTable ?
Any business usecase that requires the streaming app to maintain the latest value for a given key can benefit from KTable.

Example :
	Stock Trading App that requires to maintain the latest value for a given Stock symbol.
	
	




once the message is consumed by this KTable what it does is it keeps buffering for certain time frame and then once again time frame is exhausted then what it will do is 
it is going to take the latest value for the given key and then publish that message downstream

so in this case the KTable waits for certain time frame and buffers the record with in the time frame and once the time frame is exhausted its going to sending that message downstream so in this cases its our filter() operator , toStream() and then print() gets executed based on the data.


so KTable is an API we use when we want to see latest value for any given key

When does KTable decides to emit the data(records) to the downstream nodes ?
There are two configurations that controls this ::
	1. cache.max.bytes.buffering : this means KTable uses cache internally for de-duplication and cache serves as a buffer 
	2. commit.interval.ms 


1. cache.max.bytes.buffering : this means KTable uses cache internally for deduplication and cache serves as a buffer 
This buffer holds the previous value, any new message with the same key updates the current value.
Caching also helps with the amount of data written into the RocksDB 
cache.max.bytes.buffering=10485760(~10MB) ## o.apache.kafka.streams.StreamsConfig - StreamsConfig values:
so, if the buffer size is greater than the cache.max.bytes.buffering value then the data will be emiited to the downstream nodes. 

so this cache holds the value in the memory and once the memory is full then it will decide to send the value to the downstream nodes.
this is one way of managing the KTable to prevent sending the values to the downstream nodes immediately 


2. commit.interval.ms=30000 (30 seconds) ##  o.apache.kafka.streams.StreamsConfig - StreamsConfig values
this is another property responsible for sending the data from the KTable to the downstream nodes
KTable emits the data to the downstream processor nodes once the commit interval is exhausted.


Since we have use Materialized.as() any value in the KTable is going to be maintained in the internal changelog topic 
command :::: HERE WE CAN SEE OUR CHANGLELOG BEHIND THE SCENES ITS CREATED i.e. ktable-words-store-changelog for our application 

>.\bin\windows\kafka-topics.bat --bootstrap-server localhost:9092 --list
__consumer_offsets
greetings
greetings-spanish-uppercase
greetings-uppercase
ktable-words
ktable-words-store-changelog
orders
orders-general
orders-restaurant
words


command to see the values in this ktable-words-store-changelog topic ::::::::::::::::
 .\bin\windows\kafka-console-consumer.bat  --bootstrap-server localhost:9092 --topic  ktable-words-store-changelog  --from-beginning


C:\kafka> .\bin\windows\kafka-console-consumer.bat  --bootstrap-server localhost:9092 --topic ktable-words-store-changelog --from-beginning
Ambulance
Baby
Ambulance
Baby
Ambulance
Baby
Baby
Baby
Baby

so these are the updated values for the given key thats been published
these are different values that are the part of KTable which is present in the changelog topic 

when we restart the application what this is going to do is this is going to Kafka-topic and get
all the values for this state store which is here is words-store and its going to match with this 
changelog topic and adds the changelog topic ..



Words-Producer output ::
16:20:10.399 [main] INFO  c.m.g.producer.ProducerUtil - producerRecord : ProducerRecord(topic=ktable-words, partition=null, headers=RecordHeaders(headers = [], isReadOnly = false), key=B, value=Baby, timestamp=null)
16:20:10.401 [main] INFO  c.m.g.producer.WordsProducer - Published the alphabet message : ktable-words-0@69 

KTableStreamApp output ::
16:20:31.171 [ktable-ab67a044-caa8-4d62-a74b-053a02c988e2-StreamThread-1] INFO  c.m.g.topology.ExploreKTableTopology - wordsTable :: key :: A, value :: Ambulance
[ktable-words]: A, Ambulance
16:20:31.172 [ktable-ab67a044-caa8-4d62-a74b-053a02c988e2-StreamThread-1] INFO  c.m.g.topology.ExploreKTableTopology - wordsTable :: key :: B, value :: Baby
[ktable-words]: B, Baby


see the difference  i.e. commit.interval.ms=30000 (30 seconds)  is working and the value is passed downstream after 30 seconds



KTable Based Topology :::::

public class ExploreKTableTopology {
    public static final String KTABLE_WORDS="ktable-words";

    public static Topology buildTopology(){

        StreamsBuilder streamsBuilder = new StreamsBuilder();

        // once the message is consumed by this KTable what it does is it keeps buffering for certain time frame and then once again time frame is exhausted then what it will do is
        //it is going to take the latest value for the given key and then publish that message downstream
        //
        //so in this case the KTable waits for certain time frame and buffers the record with in the time frame and once the time frame is exhausted its going to sending
        // that message downstream so in this cases its our filter() operator , toStream() and then print() gets executed based on the data.
        KTable<String, String> wordsTable=streamsBuilder.table(KTABLE_WORDS, Consumed.with(Serdes.String(),Serdes.String())
        , Materialized.as("words-store")
        );

        wordsTable.filter((key, value) -> value.length() > 2)
                .toStream()
                .peek((key, value) -> log.info("wordsTable :: key :: {}, value :: {}",key,value))
                .print(Printed.<String,String>toSysOut().withLabel("ktable-words"));

        return streamsBuilder.build();
    }
}




GlobalKTable ::::::::::::::::

StreamsBuilder streamsBuilder = new StreamsBuilder();

KTable<String, String> wordsTable=streamsBuilder.globalTable(KTABLE_WORDS, Consumed.with(Serdes.String(),Serdes.String())
        , Materialized.as("words-global-store")
        );


GlobalKTable is very similar to KTable.
this also stores the latest value for a given key.
the operations are very limited in a GlobalKTable compare to KTable

An instance of GlobalKTable in each of these instance will have access to all the keys in each instance.
so if there are two instances then instance 1 and instance 2 has access to all the keys in the topic 


When to use KTable vs GlobalKTable ?
1. KTable is an advanced API and it supports many operators to enrich the data 
2. Use KTable if you have a larger KeySpace.
			1. This means if you have a huge number of Keys(may be in Millions).
			2. KTable is better for joins where time needs to be synchronized.
3. Use GlobalKTable if the KeySpace is smaller.


================================================================================================================================================================================================================================================
========================================================================================================================
========================================================================================================================




11 . Stateful Operations in KafkaStreams - Aggregate, Join and Windowing Events

1. Stateful operations in KAFKA Streams 

Stateful operations are one of the awesome freature of Kafka Streams.

what are the stateful operations that can be performed in Kakfa Streams ?
1. Aggregating of Event Streams 
		(a). calculating total number of orders made in a retail company.
		(b). calculating total revenue made for the company.

2. Joining Event Streams
		(a). combining data from two independent topics (event streams) based on a key.

3. Windowing
			(a). This is the concept of Grouping Data in a certain time window.
			Example : Calculate the number of orders made in an hour or a day or a weak or a month.
		

Stateful OPerators in KAFKA Streams 
1. Aggregation of Data 
		count , reduce and aggregate

2. Joining Data 
		join, leftJoin, outerJoin

3. Windowing Data
		windowedBy
		


Aggregating of Data 
Aggregation is the concept of combining multiple input values to produce one output.
Some of the example usecases :
	1. Calculating total number of orders made by the company.
	2. calculating total revenue made by the company.
	
How Aggregation works ?
Aggregations works only on Kafka Records that has non-null Keys.
1> Group Records by Key 
2> Aggregate the Records 

There are two configuration that controls this :
1. cache.max.bytes.buffering either this property value exceeds
2. commit.interval.ms or either this given propertly value  interval time frame gets exhausted

once these two above conditions met aggreations will automatically happens .

Aggregation Operators in KAFKA Streams 
We have three operators in Kafka Streams library to support aggregation 
1. count
2. reduce
3. aggregate

First Step of Aggregation is grouping the records by key. The aggregation is always performed based on the key so the key is mandatory 
then after that we perform aggregation.


1. count Operator :::
This is used to count the number of different events that share the same key 
			
			var groupedString = wordsTable
												.groupByKey(Grouped.with(Serdes.String(), Serdes.String()))
												.count(Named.as("count-per-alphabet"));
												
This will return a KTable of this type KTable<String, Long>
	String -> Key of the record ( representation of the actual key)
	Long   -> Value of the record (total number of events that are received for the particular key)
	
The aggregated value gets stored in an internal topic :::

Here A,Apple is one record and A,Alpha is another record and b,Bus is another record and so on. where A & B are key here.

count ::
Topic (key, value)			A, Apple			A, Alpha   B, Bus 
Count							[A:1]				[A:2][B:1]


	
 C:\kafka>.\bin\windows\kafka-topics.bat --bootstrap-server localhost:9092 --list
__consumer_offsets
aggregate
aggregate-KSTREAM-AGGREGATE-STATE-STORE-0000000002-changelog

See above that the internal topic has been created i.e.  >> aggregate-KSTREAM-AGGREGATE-STATE-STORE-0000000002-changelog
so this is where all the data resides actually
so when we restart our Kafka-Stream application we need to make sure what was the previous value given for the key , how does that application is going to reconstruct the value 
so it reconstructs this whole state of this topology meaning of this given key we have been reading through this kafka topic just in this case is going to be the internal kafka-topic.



07:15:37.926 [aggregate-830af070-5c60-4950-9c0b-32d67b087c29-StreamThread-1] INFO  o.a.k.clients.consumer.KafkaConsumer - [Consumer clientId=aggregate-830af070-5c60-4950-9c0b-32d67b087c29-StreamThread-1-consumer, groupId=aggregate] Requesting the log end offset for aggregate-0 in order to compute lag
[aggregate]: A, Apple
[aggregate]: A, Alligator
[aggregate]: A, Ambulance
[aggregate]: B, Bus
[aggregate]: B, Baby
[words-count-per-alphabet]: A, 3
[words-count-per-alphabet]: B, 2



07:23:35.124 [aggregate-830af070-5c60-4950-9c0b-32d67b087c29-StreamThread-1] INFO  o.a.k.s.p.internals.StreamThread - stream-thread [aggregate-830af070-5c60-4950-9c0b-32d67b087c29-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update
[aggregate]: A, Apple
[aggregate]: A, Alligator
[aggregate]: A, Ambulance
[aggregate]: B, Bus
[aggregate]: B, Baby
[words-count-per-alphabet]: A, 6
[words-count-per-alphabet]: B, 4


see the previous values gets updated from the internal kafka topic 
earlier [words-count-per-alphabet]: A, 3         and after running the producer application again (AggregateProducer) :::: [words-count-per-alphabet]: A, 6
so does the same for key B 
and the value gets updated i.e. this aggregation runs automatically





Since we have changed the code in topology from groupByKey() and used groupBy() there might be new internal Kakfa Topic gets created in the Kafka Cluster. when we query Kafka Cluster.

C:\kafka>.\bin\windows\kafka-topics.bat --bootstrap-server localhost:9092 --list
__consumer_offsets
aggregate
aggregate-KSTREAM-AGGREGATE-STATE-STORE-0000000002-changelog
aggregate-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog
aggregate-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition

earlier this is the internal KAFKA Topic we saw  :::: aggregate-KSTREAM-AGGREGATE-STATE-STORE-0000000002-changelog

now we see two additional Internal Kafka Topics ::: 
aggregate-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelog
aggregate-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition

this new internal Kafka topic is created because we have new key in groupBy() and for that new key it needs to maintain the changelog topic so new KAFKA Topic is created
and repartition is the concept where it needs to repartition the kafka records in such a way that when you are running that in a distributed environment this is going to take care of placing the records 
in the appropriate partition.

anytime we are changing the key we have two internal topics created 1. is changelog and 2. is repartition so this is needed for repartitioning the records  so that records get distributed evenly across
the Kafka Cluster


---------------------------------------------------------------------------------------------------------------------------------------------


4. Aggregation using reduce operator ::::

reduce operator ::::

This operator is used to reduce multiple values to a single value that shares the same key.

var groupedString= wordsTable
									.groupByKey(Grouped.with(Serdes.String(), Serdes.String()))
									.reduce((previousValue, currentValue) -> {
											log.info("previousValue:: {} ,  currentValue :: {}", previousValue, currentValue);
											return previousValue.toUpperCase() +  "--" + currentValue.toUpperCase();
									});

The return type of the reduce operator must be the same type as the value.

The aggregated value gets stored in an internal topic (this helps with fault Tolerance and retention)


----------------------------------------------------------------------------------------------------------------
5. Aggregation using aggregate operator ::::

aggregate::::::::
An aggregate operator is similar to reduce operator.

The aggregated type can be different form actual Kafka Records that we are acting on.

Topic (Key,Value)							A, Apple																														A, Adam
																								
aggregate							{"key" : "A", "Values" : ["Apple"],"running_count" : 1}														{	"key" : "A", "Values" : ["Apple", "Adam"], "running_count" : 2}

The aggregated value gets stored in an internal topic 


// Initializer is java bean which is going to represent JSON that has three properties and we are instatiating  new instance of it. Type here is initializer.
Initializer<AlphabetWordAggregate> alphabetWordAggregateInitializer = AlphabetWordAggregate::new;


// aggregator :: where we are updating the running_count and update the array-value with the new value as the new values come in .
Aggregator<String, String, AlphabetWordAggregate> aggregator = (key, value, alphabetWordAggregate) -> { return alphabetWordAggregate.updateNewEvents (key, Value);};
											
// materializing the aggregated value to a stateStore 
// as this is needed anytime the application is restarted and the app needs to reconstruct whole stream / state 
// the reason why Materialized is used in this usecase  not for the other one is that because the type here is going to be little different because the value is going to be new object or a new type. 
// its not going to be String anymore .that is why we are using materialized
var aggregatedStream = groupedString 
									.aggregate( alphabetWordAggregateInitializer,
									aggregator,
									Materialized.<String, AlphabetWordAggregate, KeyValueStore< Bytes, byte [] >> as("aggregated-words")
									.withKeySerde(Serdes.String())
									.withValueSerde(SerdesFactory.alphabetWordAggregate())
									);








-----------------------------------------------------------------------------------------------------------------------------------------




@Slf4j
public record AlphabetWordAggregate(String key,
                                    Set<String> valueList,
                                    int runningCount) {


    public AlphabetWordAggregate() {
        this("", new HashSet<>(), 0);
    }


    // everytime the function is invoked we are going to update the runningCount
    public AlphabetWordAggregate updateNewEvents(String key, String newValue){
        log.info("Before the update : {} , valueList :: {}", this, valueList );
        log.info("New Record : key : {} , value : {} : ", key, newValue );
        var newRunningCount = this.runningCount +1;
        valueList.add(newValue);
        var aggregated = new AlphabetWordAggregate(key, valueList, newRunningCount);
        log.info("aggregated : {}" , aggregated);
        return aggregated;
    }


    public static void main(String[] args) {


        var al =new AlphabetWordAggregate();

    }

}






public class SerdesFactory {


    // in future if we want a different type we basically create another factory function then we change the type over here
    // like here we used Greeting we can use any other type accordingly
    public static Serde<AlphabetWordAggregate> alphabetWordAggregate() {

        JSONSerializer<AlphabetWordAggregate> jsonSerializer = new JSONSerializer<>();

        JSONDeserializer<AlphabetWordAggregate> jsonDeSerializer = new JSONDeserializer<>(AlphabetWordAggregate.class);
        return  Serdes.serdeFrom(jsonSerializer, jsonDeSerializer);
    }


    public static Serde<Alphabet> alphabet() {

        JSONSerializer<Alphabet> jsonSerializer = new JSONSerializer<>();

        JSONDeserializer<Alphabet> jsonDeSerializer = new JSONDeserializer<>(Alphabet.class);
        return  Serdes.serdeFrom(jsonSerializer, jsonDeSerializer);
    }
}



















import com.mytutorial.advancedstreams.domain.AlphabetWordAggregate;
import com.mytutorial.advancedstreams.serdes.SerdesFactory;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.common.utils.Bytes;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.kstream.*;
import org.apache.kafka.streams.state.KeyValueStore;

@Slf4j
public class ExploreAggregateOperatorsTopology {


    public static final String AGGREGATE = "aggregate";

    public static Topology build(){
        StreamsBuilder streamsBuilder = new StreamsBuilder();

        KStream<String, String> inputStream = streamsBuilder
                .stream(AGGREGATE,
                        Consumed.with(Serdes.String(),Serdes.String()));

        inputStream
                .print(Printed.<String,String>toSysOut().withLabel(AGGREGATE));

        // groupByKey() takes the key automatically from the actual record and then apply grouping on it
        // so if the key is we would like to change from the actual Kafka Record that's when we use operator groupBy()
        // groupBy() accepts the KeyValueMapper basically its a key selector so the advantage is that we can provide what the new key is going to be
        // we can alter the Key Also . For some usecases there might be no key in Kafka Records Streaming from the Kafka Topic
        // in this case we have the key as A and B in some cases there might not be any key in those kind of scenarios we can use
        // groupBy() operator to change a key or even if there is an existing key and we would like to change the key to different value then use groupBy() operator
        // anytime we perform groupBy() its recommended to provide the type also >> here  Grouped.with(Serdes.String(), Serdes.String())
        KGroupedStream<String,String> groupedString = inputStream
                .groupByKey(Grouped.with(Serdes.String(), Serdes.String())) // here key is same as in Kafka Records from Internal Kafka Topic
                //        .groupBy((key, value) -> value ,                // here key is now changed value has become key and below type of key-value is provided which is recommended.
                //                Grouped.with(Serdes.String(),Serdes.String()))
                ;

         exploreCount(groupedString);
        exploreReduce(groupedString);
        // exploreAggregate(groupedString);

        return streamsBuilder.build();
    }

    private static void exploreCount(KGroupedStream<String, String> groupedStream) {

        // when we do something like this what Kafka Stream does is that it creates an internal Kafka Topic and maintains the data in the Kafka Topic
        // but there is no way to query the data if we are using this approach i.e. without using Materialized
        // if we are using Materialized then instead of creating Internal Kafka Topic of its own its going to create state store and maintain the data in the internal Kafka Topic

        KTable<String,Long> countByAlphabet = groupedStream
                //.count(Named.as("count-per-alphabet"))
                .count(Named.as("COUNT-PER-ALPHABET"),
                Materialized.as("COUNT-PER-ALPHABET-STORE"))
                ;

        countByAlphabet
                .toStream()
                .print(Printed.<String,Long>toSysOut().withLabel("words-count-per-alphabet"));

    }

    // behind the scenes Kafka Stream did create  internal Kafka Topic and stored the complete state over there
    // for FaultTolerance and retention
    private static void exploreReduce(KGroupedStream<String, String> groupedStream){
        KTable<String,String> reducedKtable=groupedStream
                                        .reduce((previousValue, currentValue) -> {
                                           log.info("PreviousValue :: {} , CurrentValue :: {}",previousValue,currentValue);
                                           return  previousValue.toUpperCase() + "-" + currentValue.toUpperCase();
                                        }
                                        , Materialized.<String,String,KeyValueStore<Bytes,byte[]>>as("REDUCED-WORDS-STORE")
                                                        .withKeySerde(Serdes.String())
                                                        .withValueSerde(Serdes.String())
                                        );
        reducedKtable.toStream()
                .print(Printed.<String, String>toSysOut().withLabel("REDUCED-WORDS"));
    }

    private static void exploreAggregate(KGroupedStream<String, String> groupedStream){
        // Initializer is java bean which is going to represent JSON that has three properties and we are instatiating  new instance of it. Type here is initializer.
        // creating new empty instance
        Initializer<AlphabetWordAggregate> alphabetWordAggregateInitializer= AlphabetWordAggregate::new;

        // this is the actual code which is going to perform aggregation
        // as we are getting new value we are calling updateNewEvents()
        //  aggregator :: where we are updating the running_count and update the array-value with the new value as the new values come in .
        Aggregator<String,String,AlphabetWordAggregate> alphabetWordAggregateAggregator=
                (key, value, alphabetWordAggregate) -> alphabetWordAggregate.updateNewEvents(key, value);

        // since we are changing the type from one to another because we are changing from type String to AlphabetWordAggregate
        // the better option is to provide Materialized . Materialized is one of the option of providing our own State Store instead of Kafka taking care to create that State Store for us.
        // Materialized.<String,AlphabetWordAggregate >as("aggregated-store") first is key second is the value  third is providing what kind of state store we are going to use.
        // materializing the aggregated value to a stateStore
// as this is needed anytime the application is restarted and the app needs to reconstruct whole stream / state
// the reason why Materialized is used in this usecase  not for the other one is that because the type here is going to be little different because the value is going to be new object or a new type.
// its not going to be String anymore .that is why we are using materialized . so in aggregate operator we have to use Materialized view but in count and reduce operator if we don't use Materialized then there won't be any problems.
        // advantage of using Materialized views when saving the state of the Aggregated Operation
        KTable<String,AlphabetWordAggregate>alphabetWordAggregateKTable=groupedStream
                                            .aggregate(alphabetWordAggregateInitializer,
                                                    alphabetWordAggregateAggregator,
                                                    Materialized.<String,AlphabetWordAggregate, KeyValueStore<Bytes,byte[]>>as("AGGREGATED-WORDS-STORE")
                                                            .withKeySerde(Serdes.String()) // specifying key serde
                                                            .withValueSerde(SerdesFactory.alphabetWordAggregate()) // specifying value serde
                                                    );
        alphabetWordAggregateKTable.toStream()
                .print(Printed.<String,AlphabetWordAggregate>toSysOut().withLabel("AGGREGATED-WORDS"));
    }

}

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

================================================================================================================================================================================================================================================
========================================================================================================================
========================================================================================================================


13. Aggregation In Order Management Application::::::::::

1. Total number of orders by each store using count operator
2. Total Revenue by each store using aggregator operator 




import com.mytutorial.orderskafkastreamsapp.domain.Order;
import com.mytutorial.orderskafkastreamsapp.domain.OrderType;
import com.mytutorial.orderskafkastreamsapp.domain.Revenue;
import com.mytutorial.orderskafkastreamsapp.domain.TotalRevenue;
import com.mytutorial.orderskafkastreamsapp.serdes.OrderSerdesFactory;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.common.utils.Bytes;
import org.apache.kafka.streams.KeyValue;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.kstream.*;
import org.apache.kafka.streams.state.KeyValueStore;

import java.math.BigDecimal;

/**
 *Command to consume from Kafka-Topic
 * .\bin\windows\kafka-console-consumer.bat  --bootstrap-server localhost:9092 --topic orders-restaurant
 * .\bin\windows\kafka-console-consumer.bat  --bootstrap-server localhost:9092 --topic orders-general
 *
 */
@Slf4j
public class OrdersTopology {
    public static final String ORDERS = "orders";

    public static final String RESTAURANT_ORDERS= "orders-restaurant";
    public static final String RESTAURANT_ORDERS_COUNT= "orders-restaurant-count";
    public static final String RESTAURANT_ORDERS_TOTAL_REVENUE= "orders-restaurant-total-revenue";
    public static final String GENERAL_ORDERS= "orders-general";
    public static final String GENERAL_ORDERS_COUNT= "orders-general-count";
    public static final String GENERAL_ORDERS_TOTAL_REVENUE= "orders-general-total-revenue";
    public static final String STORES = "stores";

    public static Topology buildTopology(){

        Predicate<? super String,? super Order> generalPredicate= (key,order) -> order.orderType().equals(OrderType.GENERAL);
        Predicate<? super String,? super Order> restaurantPredicate= (key,order) -> order.orderType().equals(OrderType.RESTAURANT);

        StreamsBuilder streamsBuilder=new StreamsBuilder();

        KStream<String, Order> orderStream= streamsBuilder.stream(ORDERS, Consumed.with(Serdes.String(), OrderSerdesFactory.orderSerde()));

        orderStream.print(Printed.<String,Order>toSysOut().withLabel(ORDERS));

        exploreOrderCount(orderStream, generalPredicate, GENERAL_ORDERS_COUNT);
        exploreOrderCount(orderStream, restaurantPredicate, RESTAURANT_ORDERS_COUNT);

        totalRevenue(orderStream,GENERAL_ORDERS_TOTAL_REVENUE,generalPredicate);
        totalRevenue(orderStream,RESTAURANT_ORDERS_TOTAL_REVENUE,restaurantPredicate);

        splitUsingBranched(orderStream, generalPredicate, restaurantPredicate);

        //mySplitUsingFilter(orderStream);


        return streamsBuilder.build();
    }

    //   tutorial way of doing the split of OrderStream into Two General and Restaurant and produce to two different kafka-topics
    private static void splitUsingBranched(KStream<String, Order> orderStream, Predicate<? super String, ? super Order> generalPredicate, Predicate<? super String, ? super Order> restaurantPredicate) {

        ValueMapper<Order,Revenue> revenueValueMapper=order -> new Revenue(order.locationId(), order.finalAmount());

        // BranchedKStream<K, V> branch(Predicate<? super K, ? super V> var1, Branched<K, V> var2);
        orderStream.split(Named.as("Restaurant_General_Orders"))
                        .branch(generalPredicate,
                        Branched.withConsumer(generalOrderStream ->{

                            generalOrderStream.print(Printed.<String,Order>toSysOut().withLabel("GENERAL-ORDER-STREAM"));

                            generalOrderStream.mapValues((readOnlyKey,order) -> revenueValueMapper.apply(order))
                            .to(GENERAL_ORDERS,Produced.with(Serdes.String(),OrderSerdesFactory.revenueSerde()));

                                }))
                                .branch(restaurantPredicate,
                                        Branched.withConsumer(restaurantOrderStream ->{

                                            restaurantOrderStream.print(Printed.<String,Order>toSysOut().withLabel("RESTAURANT-ORDER-STREAM"));

                                            restaurantOrderStream
                                                    .mapValues((readOnlyKey,order) -> revenueValueMapper.apply(order))
                                                    .to(RESTAURANT_ORDERS, Produced.with(Serdes.String(),OrderSerdesFactory.revenueSerde()));
                                        })
                                        );
    }

    // my way of doing the split of OrderStream into Two General and Restaurant
    private static void mySplitUsingFilter(KStream<String, Order> orderStream) {
        KStream<String,Revenue> restaurantOrderStream= orderStream
                                .filter((k,v)->v.orderType().equals(OrderType.RESTAURANT))
                .mapValues((readOnlyKey,order) ->{
                    String locationId= order.locationId();
                    BigDecimal revenue=order.finalAmount();
                    return new Revenue(locationId,revenue);
                });

        restaurantOrderStream.print(Printed.<String,Revenue>toSysOut().withLabel("RESTAURANT-ORDER-STREAM"));


        KStream<String,Revenue> generalOrderStream= orderStream
                                .filter((k,v)->v.orderType().equals(OrderType.GENERAL))
                .mapValues((readOnlyKey,order) ->{
                    String locationId= order.locationId();
                    BigDecimal revenue=order.finalAmount();
                    return new Revenue(locationId,revenue);
                });

        generalOrderStream.print(Printed.<String,Revenue>toSysOut().withLabel("GENERAL-ORDER-STREAM"));


        restaurantOrderStream.to(RESTAURANT_ORDERS, Produced.with(Serdes.String(), OrderSerdesFactory.revenueSerde()));

        generalOrderStream.to(GENERAL_ORDERS,Produced.with(Serdes.String(),OrderSerdesFactory.revenueSerde()));
    }

    private static void exploreOrderCount(KStream<String, Order> orderStream, Predicate<? super String,? super Order> predicateOrderType, String orderTypeCount){


        KTable<String,Long> ordersCount =
        orderStream
                .filter(predicateOrderType)
                .map((key, value) -> KeyValue.pair(value.locationId(),value))
                .groupByKey(Grouped.with(Serdes.String(),OrderSerdesFactory.orderSerde()))
                .count(Named.as(orderTypeCount), Materialized.as(orderTypeCount));

        ordersCount
                .toStream()
                .print(Printed.<String,Long>toSysOut().withLabel(orderTypeCount));

    }

    private static void totalRevenue(KStream<String, Order> orderKStream, String orderTotalRevenue, Predicate<? super String,? super Order> predicateOrderType){

        Initializer<TotalRevenue> totalRevenueInitializer= TotalRevenue::new;

        Aggregator<String, Order, TotalRevenue> totalRevenueAggregator=(key, value, totalRevenue) -> totalRevenue.updateTotalRevenue(key, value);

        KTable<String,TotalRevenue> aggregatedTotalRevenue=orderKStream
                .filter(predicateOrderType)
                .map((key, value) -> KeyValue.pair(value.locationId(),value) )
                .groupByKey(Grouped.with(Serdes.String(),OrderSerdesFactory.orderSerde()))
                .aggregate(
                        totalRevenueInitializer,
                        totalRevenueAggregator,
                        Materialized.<String, TotalRevenue,KeyValueStore<Bytes,byte[]>>as(orderTotalRevenue)
                                .withKeySerde(Serdes.String())
                                .withValueSerde(OrderSerdesFactory.totalRevenueSerde())
                );

        aggregatedTotalRevenue.toStream()
                .print(Printed.<String,TotalRevenue>toSysOut().withLabel(orderTotalRevenue));


    }

}




================================================================================================================================================================================================================================================
========================================================================================================================
========================================================================================================================

14. Re-Keying Kafka Records for Stateful operations

1. Effect of null Key in Stateful Operations & Repartition of Kafka Records

Skipping record due to null key or value.........

anytime in the records key is null then records gets skipped

when we change a key in our aggregate code logic because for groupBy() it needs to have the keys assigned to this group operation

when we have map() operation (re-keying or changing the key for key-value pair record) along with the groupBy() operation  thats when its going to perform repartition behavior behind the scenes

anytime we use Materialized view we will use changelog topic
and repartition is a topic which comes into play (which gets created internally) when you are changing the key for any record

since we added a new key i.e. we changed the key from orderId to storeId or locationId in those scenarios
the datas get written back to the this repartition topic and then the whole process of  reconstructing this key-value pair happens behind the scenes 
so when we are changing the key one of the performance impact we notice is that all the kafka records need to be written into repartition topic and then read from the repartition topic so that 
it represents latest value .




2. Re-Keying using the selectKey operator

The primary role of this operator is to rekey the Kafka records.

var orderStream = streamsBuilder
								.stream(ORDERS, Consumed.with(Serdes.String(), SerdesFactory.orderSerdes()))
								.selectKey((key, value) -> value.locationId());

selectKey operator is expensive operation because selectKey call will trigger the repartition of the records.

1. Internal re-partition topic gets created and records will be persisted in the topic for repartition
2. Streams tasks needs to read these messages again for further processing.

The effect is very similar to using "map()" operation with "groupBy()" 

This operator doesn't require you to have an aggregate operation as long as we use this operator its going to perform immediately repartition.

when se use selectKey() irrespective of the aggregate operation its going to perform re-partition

while using selectKey() we need not have to use along with the groupBy()



================================================================================================================================================================================================================================================
========================================================================================================================
========================================================================================================================


15. Stateful Operations in KAFKA Streams - Join 

1. Introduction to Joins & Types of Joins in Kafka Streams

Joining in KAFKA Streams 
joins in KAFKA Streams are used to combine data from multiple topics.
Analogy :: Joining Data from multiple tables in a Relational DB 

joins will only be performed if their a matching key in both the topics.

How is this different from merge operator ?
	Merge operator has no condition or check to combine events. i.e. there is no such condition
	for matching key in both the topics to perform joining in kafka streams .
	
	
	Join Operators in KAFKA Stream ::
	1. join 
	2. leftJoin
	3. outerJoin 
	

Types of Joins in KAFKA Streams ::
Join Types 						Operators::
KStream-KTable				join, leftJoin
KStream-GlobalKTable		join, leftJoin
KTable-KTable					join,leftJoin,outerJoin
KStream-KStream 			join, leftJoin, outerJoin

joins will get triggered if there is a matching record for the same key.
to achieve a resulting  data model like this, we would need a ValueJoiner. 
this is also called innerJoin.
Join won't happen if the records from topics don't share the same key.



	
	
2. Explore innerJoin using join operator - Joining KStream and KTable

joins will get triggered if there is a matching record for the same key.
to achieve a resulting  data model like this, we would need a ValueJoiner. 
this is also called innerJoin.
Join won't happen if the records from topics don't share the same key.


so in case of KStream with KTable 
new events into the KTable doesn't trigger any join
but new events into the KSTREAM always trigger join if there is matching key is found in KTable



OUTPUT CONSOLE::::::;

14:15:03.381 [joins1-94dbdacc-fc3c-45e3-9ad5-b96b1c8a2009-StreamThread-1] INFO  o.a.k.s.p.internals.StreamThread - stream-thread [joins1-94dbdacc-fc3c-45e3-9ad5-b96b1c8a2009-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update
[alphabets_abbreviations]: C, Cat.
[alphabets_abbreviations]: B, Bus.
[JOINED-STREAM]: B, Alphabet[abbreviation=Bus., description=B is the second letter in English Alphabets.]
[alphabets_abbreviations]: A, Apple
[JOINED-STREAM]: A, Alphabet[abbreviation=Apple, description=A is the first letter in English Alphabets.]
[alphabets]: B, B is the second letter in English Alphabets.
[alphabets]: A, A is the first letter in English Alphabets.


[alphabets_abbreviations]: B, Baby.
[JOINED-STREAM]: B, Alphabet[abbreviation=Baby., description=B is the Second letter in English Alphabets.]
[alphabets_abbreviations]: A, Airplane
[JOINED-STREAM]: A, Alphabet[abbreviation=Airplane, description=A is the First letter in English Alphabets.]
[alphabets]: B, B is the Second letter in English Alphabets.
[alphabets]: A, A is the First letter in English Alphabets.





------------------------------------------------------------------------------------------------------


3. Explore innerJoin using join operator - Joining KStream and GlobalKTable

Here it works same as KStream and KTable Joining 

when we are joining KStream with GlobalKTable then We need KeyValueMapper and ValueJoiner the reason is that GlobalKTable is the representation of all the data thats part of the KAFKA Topic 
its not about a specific instance holding set of keys based on partition that particular task interacts with its going to have whole representation in those kind of scenarios we need to provide
KeyValueMapper thats going to represent what the key is going to be in this case 

OUTPUT CONSOLE::::::::::::::::::

[alphabets_abbreviations]: A, Apple
[JOINED-STREAM]: A, Alphabet[abbreviation=Apple, description=A is the First letter in English Alphabets.]
[alphabets_abbreviations]: B, Bus.
[JOINED-STREAM]: B, Alphabet[abbreviation=Bus., description=B is the Second letter in English Alphabets.]
[alphabets_abbreviations]: C, Cat.




-----------------------------------------------------------------------------------------

4. Explore innerJoin using join operator - Joining KTable and KTable

In this usecase either side of the data is going to trigger join in our case :::: alphabets_abbreviations table and alphabet table both are KTables so irrespective of whether the data is going to
be sent to this alphabets_abbreviations table or alphabet table there will be join triggered.

 IN OUTPUT CONSOLE :: we see the data with label [JOINED-STREAM]:  is printed 4 times here this is happening because both the side of the data is triggering the join 
 in this case we have 2 KTables alphabets_abbreviations and alphabet and we publish two messages to each and based on key it already triggering the join from two  sides i.e. 2+2=4 times data is 
 getting printed so data from either side will trigger join stream 
 but this was not in the case of KStream-KTable and KStream-GlobalKTable Joining the join was only triggered if the data was present for the KStream that we are dealing with , not dependent on 
 GlobalKTable or KTable side data present there or not,  join was not triggered.
 
 
 OUTPUT CONSOLE::::::::::::::::::::::
 
 [alphabets_abbreviations]: A, Apple
[JOINED-STREAM]: A, Alphabet[abbreviation=Apple, description=A is the First letter in English Alphabets.]
[alphabets_abbreviations]: B, Bus.
[JOINED-STREAM]: B, Alphabet[abbreviation=Bus., description=B is the Second letter in English Alphabets.]
[alphabets_abbreviations]: C, Cat.
[alphabets]: A, A is the First letter in English Alphabets.
[JOINED-STREAM]: A, Alphabet[abbreviation=Apple, description=A is the First letter in English Alphabets.]
[alphabets]: B, B is the Second letter in English Alphabets.
[JOINED-STREAM]: B, Alphabet[abbreviation=Bus., description=B is the Second letter in English Alphabets.]


BELOW OUTPUT CONSOLE SHOWS THAT WHEN DATA IS FROM ONE SIDE (FROM ANY SIDE ) IS PUBLISHED THEN ALSO  Join is TRIGGERED .

[alphabets_abbreviations]: A, Apple
[JOINED-STREAM]: A, Alphabet[abbreviation=Apple, description=A is the First letter in English Alphabets.]
[alphabets_abbreviations]: B, Bus.
[JOINED-STREAM]: B, Alphabet[abbreviation=Bus., description=B is the Second letter in English Alphabets.]
[alphabets_abbreviations]: C, Cat.


[alphabets]: B, B is the Second letter in English Alphabets.
[JOINED-STREAM]: B, Alphabet[abbreviation=Bus., description=B is the Second letter in English Alphabets.]
[alphabets]: A, A is the First letter in English Alphabets.
[JOINED-STREAM]: A, Alphabet[abbreviation=Apple, description=A is the First letter in English Alphabets.]



---------------------------------------------------------------------------------------------


5. Explore innerJoin using join operator - Joining KStream and KStream

Join KStream - KStream ::::
The KStream-KStream join is little different compared to the other ones.
A KStream is an infinite stream which represents a log of everything that happened 

It is expected that they both share the same key  and also it should be in certain time window( there is a time window defined within the time window those events should be part of the KStream events ) 

so by default any records that gets produced in the KAFKA Topic gets a timestamp attached to it 


what is the type of join-params 
StreamJoined<K,V1,V2> this class using which we can provide what the key-value and returned type is going to be 

if the primary stream begins a window within the 5 second window (here 5 second window is specified as JoinWindows ) which is back and forth which means like if the time is 5:00:00 of the event in primary stream then secondary stream event comes at 4:59:56 (4pm59 minutes and 56 seconds) and 5:00:04 (5 pm 00 minutes and 04 seconds )within that window back and forth 
if the event comes in the secondary stream then two events or messages from two KStreams will be joined when they both have same matching key 

once the time window exhausted join will not triggered.


OUTPUT CONSOLE ::::::

OUTPUT without any delay ::: here is no delay introduced in alphabet topic  while publishing the records to topics i.e. there is no back-n-forth delay in alphabet topic .

[alphabets]: A, A is the first letter in English Alphabets.
[alphabets]: B, B is the second letter in English Alphabets.
[alphabets_abbreviations]: B, Bus
[JOINED-STREAM]: B, Alphabet[abbreviation=Bus, description=B is the second letter in English Alphabets.]
[alphabets_abbreviations]: C, Cat
[alphabets_abbreviations]: A, Apple
[JOINED-STREAM]: A, Alphabet[abbreviation=Apple, description=A is the first letter in English Alphabets.]
07:33:16.884 [joins1-94dbdacc-fc3c-45e3-9ad5-b96b1c8a2009-StreamThread-1] INFO  o.a.k.s.p.internals.StreamThread - stream-thread [joins1-94dbdacc-fc3c-45e3-9ad5-b96b1c8a2009-StreamThread-1] Processed 10 total records, ran 0 punctuators, and committed 2 total tasks since the last update


OUTPUT with delay of +4 second 

07:57:34.386 [joins1-94dbdacc-fc3c-45e3-9ad5-b96b1c8a2009-StreamThread-1] INFO  o.a.k.s.p.internals.StreamThread - stream-thread [joins1-94dbdacc-fc3c-45e3-9ad5-b96b1c8a2009-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update
[alphabets::07:55:33.418619400]: A, A is the first letter in English Alphabets.
[alphabets::07:55:33.418619400]: B, B is the second letter in English Alphabets.
[alphabets_abbreviations::07:55:33.417619900]: B, Bus
[JOINED-STREAM::07:55:33.427626100]: B, Alphabet[abbreviation=Bus, description=B is the second letter in English Alphabets.]
[alphabets_abbreviations::07:55:33.417619900]: C, Cat
[alphabets_abbreviations::07:55:33.417619900]: A, Apple
[JOINED-STREAM::07:55:33.427626100]: A, Alphabet[abbreviation=Apple, description=A is the first letter in English Alphabets.]

OUTPUT with delay of -4 second 

[alphabets::07:55:33.418619400]: B, B is the second letter in English Alphabets.
[alphabets::07:55:33.418619400]: A, A is the first letter in English Alphabets.
[alphabets_abbreviations::07:55:33.417619900]: A, Apple
[JOINED-STREAM::07:55:33.427626100]: A, Alphabet[abbreviation=Apple, description=A is the first letter in English Alphabets.]
[alphabets_abbreviations::07:55:33.417619900]: B, Bus
[JOINED-STREAM::07:55:33.427626100]: B, Alphabet[abbreviation=Bus, description=B is the second letter in English Alphabets.]
[alphabets_abbreviations::07:55:33.417619900]: C, Cat


OUTPUT with delay of +6 second

[alphabets::07:55:33.418619400]: B, B is the second letter in English Alphabets.
08:04:49.900 [kafka-producer-network-thread | joins1-94dbdacc-fc3c-45e3-9ad5-b96b1c8a2009-StreamThread-1-producer] INFO  o.apache.kafka.clients.NetworkClient - [Producer clientId=joins1-94dbdacc-fc3c-45e3-9ad5-b96b1c8a2009-StreamThread-1-producer] Node -1 disconnected.
[alphabets::07:55:33.418619400]: A, A is the first letter in English Alphabets.
[alphabets_abbreviations::07:55:33.417619900]: A, Apple
[alphabets_abbreviations::07:55:33.417619900]: B, Bus
[alphabets_abbreviations::07:55:33.417619900]: C, Cat


OUTPUT with delay of -6 second

[alphabets::07:55:33.418619400]: B, B is the second letter in English Alphabets.
[alphabets::07:55:33.418619400]: A, A is the first letter in English Alphabets.
[alphabets_abbreviations::07:55:33.417619900]: B, Bus
[alphabets_abbreviations::07:55:33.417619900]: A, Apple
[alphabets_abbreviations::07:55:33.417619900]: C, Cat



-----------------------------------------------------------------------------------------------------------------

6. Joining Kafka Streams using leftJoin operator

LeftJoin 
Join is triggered when a record on the left sie of the join is received.

var joinedStream = alphabetsAbbreviation.leftJoin(
											alphabetsStream,
											valueJoiner,
											tenSecondWindow,
											streamJoined
											);

here alphabetsAbbreviation is left side stream where when a record is received it will trigger join 

if there is no matching record on the right side, then the join will be triggered with null value for the right side value.

Publish the messages one more time.
You should be able to see the joined record with "null" description for record with key "A" and "B"



OUTPUT CONSOLE::::::::::::::

[alphabets]: B, B is the Second letter in English Alphabets.
[alphabets]: A, A is the First letter in English Alphabets.
[alphabets_abbreviations]: A, Apple
[JOINED-STREAM]: A, Alphabet[abbreviation=Apple, description=A is the First letter in English Alphabets.]
[alphabets_abbreviations]: B, Bus.
[JOINED-STREAM]: B, Alphabet[abbreviation=Bus., description=B is the Second letter in English Alphabets.]
[alphabets_abbreviations]: C, Cat.




[alphabets_abbreviations]: C, Cat.
[JOINED-STREAM]: C, Alphabet[abbreviation=Cat., description=null]
[alphabets_abbreviations]: B, Bus.
[alphabets_abbreviations]: A, Apple

HERE ONE SIDE i.e. alphabet side record or data is not arrived or present so data or record on the left side i.e. alphabetsAbbreviation triggers join and for the other side null value 
[alphabets_abbreviations]: A, Apple
[JOINED-STREAM]: C, Alphabet[abbreviation=Cat., description=null]
[JOINED-STREAM]: B, Alphabet[abbreviation=Bus., description=null]
[JOINED-STREAM]: A, Alphabet[abbreviation=Apple, description=null]
[alphabets_abbreviations]: B, Bus.
[alphabets_abbreviations]: C, Cat.



7. Joining Kafka Streams using outerJoin operator

outerJoin
Join will be triggered if there is a record on either side of the join .

var joinedStream = alphabetsAbbreviation.outerJoin(alphabetsStream,
								valueJoiner,
								tenSecondWindow,
								streamJoined
								);

when a record is received on either side of the join, and if there is no matching record on the other side then the join will populate the null value to the combined result .

								
[alphabets]: A, A is the First letter in English Alphabets.
[alphabets]: B, B is the Second letter in English Alphabets.
[alphabets_abbreviations]: A, Apple
[JOINED-STREAM]: A, Alphabet[abbreviation=Apple, description=A is the First letter in English Alphabets.]
[alphabets_abbreviations]: B, Bus.
[JOINED-STREAM]: B, Alphabet[abbreviation=Bus., description=B is the Second letter in English Alphabets.]
[alphabets_abbreviations]: C, Cat.	
								


[alphabets]: A, A is the First letter in English Alphabets.
[alphabets]: A, A is the First letter in English Alphabets.
[JOINED-STREAM]: B, Alphabet[abbreviation=null, description=B is the Second letter in English Alphabets.]
[JOINED-STREAM]: A, Alphabet[abbreviation=null, description=A is the First letter in English Alphabets.]
[alphabets]: B, B is the Second letter in English Alphabets.


								

8. Join - Under the hood
					
the internal kafka-topic creates behind the scenes  so here we are trying to have some control over  the creation of these Kafka Internal topics .	

we can do this using StreamJoined i.e.
 var joinedParams = StreamJoined.with(Serdes.String(), Serdes.String(), Serdes.String())
									.withName("alphabets-join")
									.withStoreName("alphabets-join");
				
this is helpful when the code that drastically changes or changed this way we will be able to refer to old state-store and we will be able to retain state from that .




9. CoPartitioning Requirements in Joins

the source topics that involved in the join should have the same number of partitions in the related topics

1. The source topics used for joins should have the same number of partitions.
	Each partition is assigned to a task in KAFKA Streams.
    This guarantees that the related tasks are together and join will work as expected.
	we can use selectKey or map operator to meet these requirements (in some scenarios number of partitions in source topics involved might differ then we can use selectKey() or map() means to 	re-key the records so that the records are going to be same part of the partitions)
2. Related records should be keyed on the same key.
		same partition strategy should be used when we are publishing the records into the kafka topic.
		

CO-PARTITIONING PRE-REQUISITE REQUIREMENTS in BELOW JOIN TYPES :::: 

Join Types 								CoPartitioning 				Operators
KStream-KStream					Yes								join, leftJoin, outerJoin
KStream-KTable						Yes 								join, leftJoin
KTable-KTable							Yes								join, leftJoin, outerJoin
KStream-GlobalKTable				No								join, leftJoin


They are not been joined at all the reason why it is not happening its because they all are published into different topics so if we take a look into it  in alphabets_abbreviations
there are three partitions and three tasks created where as for alphabet there is only
one partition and one task created . since alphabets_abbreviations has three tasks and three 
partitions data is getting published differently i.e. in different - different partitions as records 
are evenly distributed across all the kafka topic partitions and  two source topics used for joins have different number of partitions so join was not triggered


OUTPUT CONSOLE ::: Join is not happening because two source topics have different number of partitions even though key for the related record is same.
	 

[alphabets::12:19:42.658669]: A, A is the First letter in English Alphabets.
[alphabets::12:19:42.658669]: B, B is the Second letter in English Alphabets.
[alphabets_abbreviations::12:19:42.656668600]: B, Bus.
[alphabets_abbreviations::12:19:42.656668600]: C, Cat.
[alphabets_abbreviations::12:19:42.656668600]: A, Apple



--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


KStream-KTable::::			
ValueJoiner


KStream-GlobalKTable::::
ValueJoiner, KeyValueMapper


KTable-KTable::::
ValueJoiner


KStream-KStream:::
ValueJoiner, JoinWindows, StreamJoined




/**
 *Command to consume from Kafka-Topic
 * .\bin\windows\kafka-console-consumer.bat  --bootstrap-server localhost:9092 --topic orders-restaurant
 * .\bin\windows\kafka-console-consumer.bat  --bootstrap-server localhost:9092 --topic orders-general
 *
 * The primary purpose of a ValueMapper is to transform the values of records within a Kafka stream.
 * When processing a stream, you might need to modify or transform the values of the records according to your application's logic.
 * Kafka Streams offers various operations like mapValues(), flatMapValues(), transformValues(), etc., that accept a ValueMapper.
 * These operations apply the ValueMapper function to each record's value within the stream.
 * Example Scenario:
 * Let's consider an example where you have an input stream of Order objects and you want to transform each Order object into a Revenue object.
 * You would define a ValueMapper<Order, Revenue> where the input type is Order and the output type is Revenue.
 * Inside the ValueMapper, you would implement the logic to extract relevant information from the Order object and construct a Revenue object.
 *
 * public class OrderToRevenueMapper implements ValueMapper<Order, Revenue> {
 *     @Override
 *     public Revenue apply(Order order) {
 *         // Extract relevant information from the order and construct a revenue object
 *         double revenueAmount = order.getAmount() * order.getPrice();
 *         return new Revenue(order.getId(), revenueAmount);
 *     }
 * }
 *
 */

/**
     * StreamsBuilder is used to construct a topology of Kafka Streams.
     *
     * alphabetKStream is created by consuming messages from the Kafka topic named ALPHABETS using the stream() method of StreamsBuilder.
     * It's configured to deserialize keys and values as strings.
     * Similarly, alphabetAbbrevationsKStream is created by consuming messages from the Kafka topic named ALPHABETS_ABBREVATIONS
     *
     * ValueJoiner Definition:
     *
     * A ValueJoiner named alphabetValueJoiner is defined. It's a functional interface used to merge the values of the two streams into an instance of Alphabet.
     * The Alphabet::new method is likely a constructor reference used to create an Alphabet object.
     *
     * Configuring Serdes (Serializer/Deserializer):
     * Kafka Streams requires Serdes to serialize and deserialize keys and values when reading from and writing to topics.
     * StreamJoined is used to specify the Serdes for keys and values of both input streams.
     * In the code, Serdes.String() is used for both keys and values, indicating that the keys and values of the streams are expected to be strings.
     *
     * StreamJoined Configuration:
     * StreamJoined is a builder class that configures parameters for joining kafka streams
     * It's configured with key, value, and store serdes (serializer/deserializer) using Serdes.String() for both keys and values.
     * withName() and withStoreName() methods are used to name the joined stream and specify the store name respectively
     * withName(ALPHABET_TOPIC) is used to set the name of the joined stream. This name will be used internally within Kafka Streams.
     * Kafka Streams allows you to store intermediate results of stream processing in state stores. so specify the store-name to store intermediate results of stream processing
     *
     * Join Window Configuration:
     * JoinWindows defines the window settings for the join operation. In this case, a window of 5 seconds is set with no grace period
     *
     * outerJoin()  takes the following parameters
     * alphabetValueJoiner: A function that merges values from both streams into an Alphabet object.
     * fiveSecondWindow: The window configuration for the join operation.
     * paramJoins: Parameters for joining streams, including serdes and store names.
     *
	 * KeyValueMapper is a functional interface provided by Kafka Streams. It defines a method apply that takes two parameters - a key and a value - and returns a new key.
     * public interface KeyValueMapper<K, V, R> {
     *     R apply(K key, V value);
     * }
     * The join operation combines each record from the KStream (alphabetAbbrevationsKStream) with the corresponding record from the GlobalKTable (alphabetKTable).
     * However, for this join operation, a mapper is needed to match records from the KStream with records from the GlobalKTable.
     * The KeyValueMapper is used to specify how the key from the KStream (alphabetAbbrevationsKStream) should be matched with the key from the GlobalKTable (alphabetKTable).
     *
     *  KeyValueMapper<String,String,String> keyValueMapper= (leftKey, rightKey) -> leftKey;
     *  This lambda simply takes the key from the left side (KStream) and uses it as the key for the join operation.
     *  means that the records from the KStream are joined with the records from the GlobalKTable based solely on the key from the KStream.
     *  KeyValueMapper is crucial in specifying how keys should be matched between a KStream and a GlobalKTable in the join operation.
     *
     *  KeyValueMapper<String,String,String> keyValueMapper= (leftKey, rightKey) -> {
     *             if (leftKey.equals(rightKey)) return leftKey;
     *             else return  rightKey;
     *         };
     *
     * @param streamsBuilder
     */




import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.common.utils.Bytes;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.kstream.*;
import org.apache.kafka.streams.state.KeyValueStore;

import java.time.Duration;
import java.time.LocalTime;

@Slf4j
public class ExploreJoinsOperatorsTopology {


    public static final String ALPHABETS = "alphabets"; // A => First letter in the english alphabet
    public static final String ALPHABETS_ABBREVATIONS = "alphabets_abbreviations"; // A=> Apple

    public static final String ALPHABET_TOPIC="alphabets-join";

    public static final String JOINED_STREAM="JOINED-STREAM";

    private ExploreJoinsOperatorsTopology(){}


    public static Topology build(){
        StreamsBuilder streamsBuilder = new StreamsBuilder();
        // joinKStreamWithKTable(streamsBuilder);
        // joinKStreamWithGlobalKTable(streamsBuilder);
        // joinKTableWithKTable(streamsBuilder);
         joinKStreamWithKStream(streamsBuilder);
        // joinKStreamWithKStreamWithLeftJOIN(streamsBuilder);
        // joinKStreamWithKStreamWithOuterJOIN(streamsBuilder);
        return streamsBuilder.build();
    }

    /**
     * joins will get triggered if there is a matching record for the same key.
     * to achieve a resulting  data model like this, we would need a ValueJoiner.
     * this is also called innerJoin.
     * Join won't happen if the records from topics don't share the same key.
     *
     *
     * so in case of KStream with KTable
     * new events into the KTable doesn't trigger any join
     * but new events into the KSTREAM always trigger join if there is matching key is found in KTable
     * @param streamsBuilder
     */
    private static void joinKStreamWithKTable(StreamsBuilder streamsBuilder){

        var alphabetAbbrevationsKStream=streamsBuilder
                        .stream(ALPHABETS_ABBREVATIONS, Consumed.with(Serdes.String(), Serdes.String()));

        alphabetAbbrevationsKStream
                .print(Printed.<String,String>toSysOut().withLabel(ALPHABETS_ABBREVATIONS));


        var alphabetKTable=streamsBuilder
                .table(ALPHABETS, Consumed.with(Serdes.String(), Serdes.String()),
                        Materialized.as("alphabets-store"));

        alphabetKTable
                .toStream()
                .print(Printed.<String, String>toSysOut().withLabel(ALPHABETS));


        //<V1> – first value type <V2> – second value type <VR> – joined value type
        ValueJoiner<String, String, Alphabet> alphabetValueJoiner= Alphabet::new;

        KStream<String,Alphabet> joinedStream=alphabetAbbrevationsKStream
                                    .join(alphabetKTable,alphabetValueJoiner);

        // [alphabets-with-abbreviations]: A, Alphabet[abbreviation=Apple, description=A is the first letter in English Alphabets.]
        // [alphabets-with-abbreviations]: B, Alphabet[abbreviation=Bus, description=B is the second letter in English Alphabets.]
        joinedStream
                .print(Printed.<String, Alphabet>toSysOut().withLabel(""));
    }

    /**
     * Here it works same as KStream and KTable Joining
     * However when we are joining KStream with GlobalKTable then We need KeyValueMapper and ValueJoiner the reason is that GlobalKTable is the representation of all the data thats part of the KAFKA Topic
     * it's not about a specific instance holding set of keys based on partition that particular task interacts with it's going to have whole representation in those kind of scenarios we need to provide
     * KeyValueMapper that's going to represent what the key is going to be in this case
     * @param streamsBuilder
     */

    private static void joinKStreamWithGlobalKTable(StreamsBuilder streamsBuilder){

        var alphabetAbbrevationsKStream=streamsBuilder
                .stream(ALPHABETS_ABBREVATIONS, Consumed.with(Serdes.String(), Serdes.String()));

        alphabetAbbrevationsKStream
                .print(Printed.<String,String>toSysOut().withLabel(ALPHABETS_ABBREVATIONS));


        var alphabetKTable=streamsBuilder
                .globalTable(ALPHABETS, Consumed.with(Serdes.String(), Serdes.String()),
                        Materialized.as("alphabets-store"));

        // GlobalKTable has no toStream() method
        /*alphabetKTable
                .toStream()
                .print(Printed.<String, String>toSysOut().withLabel(ALPHABETS));*/


        /**
         * when we are joining KStream with GlobalKTable then We need KeyValueMapper and ValueJoiner the reason is that GlobalKTable is the representation of all the data thats part of the KAFKA Topic
         * its not about a specific instance holding set of keys based on partition that particular task interacts with its going to have whole representation in those kind of scenarios we need to provide
         * KeyValueMapper thats going to represent what the key is going to be in this case
         */
        // <K> – key type <V> – value type <VR> – mapped value type ; :::: here below this leftKey is from KStream i.e. alphabetAbbrevationsKStream
        KeyValueMapper<String,String,String> keyValueMapper= (leftKey, rightKey) -> leftKey;

        //<V1> – first value type <V2> – second value type <VR> – joined value type
        ValueJoiner<String, String, Alphabet> alphabetValueJoiner= Alphabet::new;

        KStream<String,Alphabet> joinedStream=alphabetAbbrevationsKStream
                .join(alphabetKTable,keyValueMapper,alphabetValueJoiner);

        // [alphabets-with-abbreviations]: A, Alphabet[abbreviation=Apple, description=A is the first letter in English Alphabets.]
        // [alphabets-with-abbreviations]: B, Alphabet[abbreviation=Bus, description=B is the second letter in English Alphabets.]
        joinedStream
                .print(Printed.<String, Alphabet>toSysOut().withLabel(JOINED_STREAM));
    }

    /**
     * In this usecase either side of the data is going to trigger join in our case :::: alphabets_abbreviations table and alphabet table both are KTables so irrespective of whether the data is going to
     * be sent to this alphabets_abbreviations table or alphabet table there will be join triggered.
     * @param streamsBuilder
     */
    private static void joinKTableWithKTable(StreamsBuilder streamsBuilder){

        KTable<String,String> alphabetAbbrevationsKTable=streamsBuilder
                .table(ALPHABETS_ABBREVATIONS, Consumed.with(Serdes.String(), Serdes.String()),
                        Materialized.<String, String, KeyValueStore<Bytes,byte[]>>as("alphabets-abbreviations-store"));

        alphabetAbbrevationsKTable
                .toStream()
                .print(Printed.<String,String>toSysOut().withLabel(ALPHABETS_ABBREVATIONS));


        KTable<String,String> alphabetKTable=streamsBuilder
                .table(ALPHABETS, Consumed.with(Serdes.String(), Serdes.String()),
                        Materialized.as("alphabets-store"));

        alphabetKTable
                .toStream()
                .print(Printed.<String, String>toSysOut().withLabel(ALPHABETS));


        //<V1> – first value type <V2> – second value type <VR> – joined value type
        ValueJoiner<String, String, Alphabet> alphabetValueJoiner= Alphabet::new;

        KTable<String,Alphabet> joinedStream=alphabetAbbrevationsKTable
                .join(alphabetKTable,alphabetValueJoiner);

        // [alphabets-with-abbreviations]: A, Alphabet[abbreviation=Apple, description=A is the first letter in English Alphabets.]
        // [alphabets-with-abbreviations]: B, Alphabet[abbreviation=Bus, description=B is the second letter in English Alphabets.]
        joinedStream
                .toStream()
                .print(Printed.<String, Alphabet>toSysOut().withLabel(JOINED_STREAM));
    }

    private static void joinKStreamWithKStream(StreamsBuilder streamsBuilder){
        var alphabetAbbrevationsKStream=streamsBuilder
                .stream(ALPHABETS_ABBREVATIONS, Consumed.with(Serdes.String(), Serdes.String()));

        alphabetAbbrevationsKStream
                .print(Printed.<String,String>toSysOut().withLabel(ALPHABETS_ABBREVATIONS+"::"+streamTimeStamp()));


        var alphabetKStream=streamsBuilder
                .stream(ALPHABETS, Consumed.with(Serdes.String(), Serdes.String()));

        alphabetKStream
                .print(Printed.<String, String>toSysOut().withLabel(ALPHABETS+"::"+streamTimeStamp()));

        /**
         * Join KStream - KStream ::::
         * The KStream-KStream join is little different compared to the other ones.
         * A KStream is an infinite stream which represents a log of everything that happened
         *
         * JoinWindows:::
         * It is expected that they both share the same key, and also it should be in certain time window( there is a time window defined within the time window those events should be part of the KStream events )
         *
         * so by default any records that gets produced in the KAFKA Topic gets a timestamp attached to it
         *
         *
         * what is the type of join-params
         * StreamJoined<K,V1,V2> this class using which we can provide what the key-value and returned type is going to be
         *
         * if the primary stream begins a window within the 5-second window (here 5-second window is specified as JoinWindows ) which is back and forth which means like if the time is 5:00:00 of the event in primary stream then secondary stream event comes at 4:59:56 (4pm59 minutes and 56 seconds) and 5:00:04 (5 pm 00 minutes and 04 seconds )within that window back and forth
         * if the event comes in the secondary stream then two events or messages from two KStreams will be joined when they both have same matching key
         */

        //<V1> – first value type <V2> – second value type <VR> – joined value type
        ValueJoiner<String, String, Alphabet> alphabetValueJoiner= Alphabet::new;

        JoinWindows fiveSecondWindow=JoinWindows.ofTimeDifferenceWithNoGrace(Duration.ofSeconds(5));

        /**
         * Class used to configure the name of the join processor, the repartition topic name, state stores or state store names in Stream-Stream join.
         * Type parameters: * <K> – the key type <V1> – this value type <V2> – other value type
         *
         * StreamJoined is a class that provides utility methods to define the serdes (serializer/deserializer) used when joining two KStreams or KTables.
         * Serdes.String() is a built-in serde for handling strings. It’s being used here for both the keys and values of the streams or tables being joined.
         * StreamJoined.with() is a static factory method that creates a new StreamJoined instance with the specified key, value, and other serdes.
         * In this case, streamJoined is an instance of StreamJoined configured to use String serdes for both keys and values of the joining streams/tables.
         *
         */
        StreamJoined<String, String, String> streamJoined = StreamJoined.with(Serdes.String(), Serdes.String(), Serdes.String());

        KStream<String,Alphabet> joinedStream=alphabetAbbrevationsKStream
                .join(alphabetKStream,alphabetValueJoiner,fiveSecondWindow,streamJoined);

        joinedStream
                .print(Printed.<String, Alphabet>toSysOut().withLabel(JOINED_STREAM+"::"+streamTimeStamp()));

    }

    private static void joinKStreamWithKStreamWithLeftJOIN(StreamsBuilder streamsBuilder){
        KStream<String,String> alphabetKStream=streamsBuilder
                                        .stream(ALPHABETS,
                                            Consumed.with(Serdes.String(),Serdes.String()));

        alphabetKStream
                .print(Printed.<String,String>toSysOut().withLabel(ALPHABETS));

        KStream<String,String>  alphabetAbbrevationsKStream=streamsBuilder
                                                        .stream(ALPHABETS_ABBREVATIONS,
                                                                Consumed.with(Serdes.String(),Serdes.String()));

        alphabetAbbrevationsKStream
                .print(Printed.<String,String>toSysOut().withLabel(ALPHABETS_ABBREVATIONS));

        ValueJoiner<String,String,Alphabet> alphabetValueJoiner=Alphabet::new;

        StreamJoined<String,String,String> paramJoins=StreamJoined.with(Serdes.String(),Serdes.String(),Serdes.String())
                .withName("alphabets-join")
                .withStoreName("alphabets-join");

        JoinWindows fiveSecondWindow=JoinWindows.ofTimeDifferenceWithNoGrace(Duration.ofSeconds(5));

        KStream<String,Alphabet> joinedStream=alphabetAbbrevationsKStream.leftJoin(alphabetKStream,
                                                alphabetValueJoiner,
                                                fiveSecondWindow,
                                                paramJoins);

        joinedStream
                .print(Printed.<String,Alphabet>toSysOut().withLabel(JOINED_STREAM));

    }

    private static void joinKStreamWithKStreamWithOuterJOIN(StreamsBuilder streamsBuilder){
        KStream<String,String> alphabetKStream=streamsBuilder
                .stream(ALPHABETS,
                        Consumed.with(Serdes.String(),Serdes.String()));

        alphabetKStream
                .print(Printed.<String,String>toSysOut().withLabel(ALPHABETS));

        KStream<String,String>  alphabetAbbrevationsKStream=streamsBuilder
                .stream(ALPHABETS_ABBREVATIONS,
                        Consumed.with(Serdes.String(),Serdes.String()));

        alphabetAbbrevationsKStream
                .print(Printed.<String,String>toSysOut().withLabel(ALPHABETS_ABBREVATIONS));

        ValueJoiner<String,String,Alphabet> alphabetValueJoiner=Alphabet::new;

        StreamJoined<String,String,String> paramJoins=StreamJoined.with(Serdes.String(),Serdes.String(),Serdes.String())
                .withName(ALPHABET_TOPIC)
                .withStoreName(ALPHABET_TOPIC);

        JoinWindows fiveSecondWindow=JoinWindows.ofTimeDifferenceWithNoGrace(Duration.ofSeconds(5));

        KStream<String,Alphabet> joinedStream=alphabetAbbrevationsKStream.outerJoin(alphabetKStream,
                alphabetValueJoiner,
                fiveSecondWindow,
                paramJoins);

        joinedStream
                .print(Printed.<String,Alphabet>toSysOut().withLabel(JOINED_STREAM));

    }

    private static String streamTimeStamp(){
        LocalTime now = LocalTime.now();
        return now.toString();
    }


}








================================================================================================================================================================================================================================================
========================================================================================================================
========================================================================================================================


17. StateFul Operations in Kafka Streams - Windowing


1. Introduction to Windowing and time concepts

Windowing means Grouping the records together based on the certain defined time windows

Group events by time windows/buckets that share the same key.
1. Calculating total Number of orders placed every minute/hour/day.
2. Calculating total Revenue generated every minute/hour/day

In order to group events by time we need to extract the time from the Kafka record.

Time Concepts:
Event Time::
Represents the time the record gets produced from the Kafka Producer
this time is set by the KafkaProducer.

Ingestion Time::
Represents the time the record gets appended to the Kafka topic in the log.

Processing Time::
This is the time the record gets processed or read by the consumer.
this is also called consumer processing time.


TimeStamp Extractor in Kafka Streams::::
1. FailOnInvalidTimestamp:: This is default timestamp extractor
This implementation extracts the timestamp from the Consumer Record.
Throws a StreamException if the timestamp is invalid.

2. LogAndSkipOnInvalidTimestamp
Parses the record timestamp, similar to the implementation of FailOnInvalidTimestamp
Simply logs the error incase of an invalid timestamp.

3. WallclockTimestampExtractor::
This ignores the timestamp from the consumer record and just uses the time the record got processed by the KafkaStreams Application. (here returning systems current time machine timing)


you can look into console for ::    12:59:57.591 [main] INFO  o.apache.kafka.streams.StreamsConfig - StreamsConfig values: 
default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp


Window Types:::
1. Tumbling Window
2. Hopping Window
3. SlidingWindow
4. Session Window
5. Sliding Join Window
6. Sliding Aggregation Window


1. Tumbling Window :::
This type is a fixed sized window.
Windows never overlap
Windows group records by matching keys.
Time Windows/Buckets are created from the clock of the apps running machine.

Duration windowSize = Duration.ofSeconds(5);
TimeWindows tumblingWindow = TimeWindows.ofSizeWithNoGrace(windowSize);

wordsStream
	.groupByKey()
	.windowedBy(tumblingWindow)
	.count();



RealTime Examples::
Entertainment ::
Total number of tickets sold on the opening day.
Total revenue made on the opening day.

We can achieve this by defining a tumbling window for a day.
Duration windowSize = Duration.ofDays(1);
TimeWindows tumblingWindow = TimeWindows.ofSizeWithNoGrace(windowSize);


TumblingWindow creates a time interval based on the duration that we define and it is also based on the running machines wall clock time.
it always start with 0th second and it continues to create those windows for future events that comes in this given kafka topic.

where is this timestamp comming from ?
because when we look at the Kafka Producer we are just publishing the record. in the below method publishMessageSync() we are just creating the ProducerRecord
and it is able to determine timestamp .
the default.timestamp.extractor = FailOnInvalidTimestamp
FailOnInvalidTimestamp this has ExtractRecordMetadataTimestamp
this is the class which actually extracts timestamp from this record.
so when we produce the data even though we are not passing the timestamp while publishing the data at the producer the timestamp gets added before it gets published into the kafka topic.
so before records gets published its going to take the current machine timestamp and then publish the record to the kafka topic 
that is how it is able to read the record and then extract the timestamp by executing the method extract() of ExtractRecordMetadataTimestamp


static KafkaProducer<String, String> producer = new KafkaProducer<String, String>(producerProps());

    public static Map<String, Object> producerProps(){

        Map<String,Object> propsMap = new HashMap<>();
        propsMap.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        propsMap.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        propsMap.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        return propsMap;

    }  

public static RecordMetadata publishMessageSync(String topicName, String key, String message ){

        ProducerRecord<String,String> producerRecord = new ProducerRecord<>(topicName, key, message);
        RecordMetadata recordMetadata=null;

        try {
            log.info("producerRecord : " + producerRecord);
            recordMetadata = producer.send(producerRecord).get();
        } catch (InterruptedException e) {
            log.error("InterruptedException in  publishMessageSync : {}  ", e.getMessage(), e);
        } catch (ExecutionException e) {
            log.error("ExecutionException in  publishMessageSync : {}  ", e.getMessage(), e);
        }catch(Exception e){
            log.error("Exception in  publishMessageSync : {}  ", e.getMessage(), e);
        }
        return recordMetadata;
    }
    
    

abstract class ExtractRecordMetadataTimestamp implements TimestampExtractor {

    /**
     * Extracts the embedded metadata timestamp from the given {@link ConsumerRecord}.
     *
     * @param record a data record
     * @param partitionTime the highest extracted valid timestamp of the current record's partition˙ (could be -1 if unknown)
     * @return the embedded metadata timestamp of the given {@link ConsumerRecord}
     */
    @Override
    public long extract(final ConsumerRecord<Object, Object> record, final long partitionTime) {
        final long timestamp = record.timestamp();

        if (timestamp < 0) {
            return onInvalidTimestamp(record, timestamp, partitionTime);
        }

        return timestamp;
    }

    /**
     * Called if no valid timestamp is embedded in the record meta data.
     *
     * @param record a data record
     * @param recordTimestamp the timestamp extractor from the record
     * @param partitionTime the highest extracted valid timestamp of the current record's partition˙ (could be -1 if unknown)
     * @return a new timestamp for the record (if negative, record will not be processed but dropped silently)
     */
    public abstract long onInvalidTimestamp(final ConsumerRecord<Object, Object> record,
                                            final long recordTimestamp,
                                            final long partitionTime);
}






Controlling Emission of Intermediate Results::::::::

By default, aggregated window results are emitted based on the commit.interval.ms

but what we want is anytime the defined window we have for the window is completed we want the results emitted downstream.

suppress:::
	This operator can be used to buffer the records until the time interval for the window is complete.
	in order to achieve this we need three configs :: Suppression Config, Buffer Config and BufferFull Config.

1. Suppression Config :::::::::::::
1. Suppressed.untilWindowCloses
 	only emit the results after the defined window is exhausted.
2. Suppressed.untilTimeLimit
	 only emit the results if there are no successive events for the defined time.



Suppression config for Buffer ::
2. Buffer Config:
	1. BufferConfig.maxBytes()
		Represents the total number of bytes the buffer can hold
	2. BufferConfig.maxRecords()
		Represents the total number of records the buffer can hold
        3. BufferConfig.unbounded()
		Represents unbounded memory to hold the records.if the memory is full then this will throw out of memory exception



3. BufferFull Strategy:
	1. shutDownWhenFull
		Shutdown the app if the buffer is full
	2. emitEarlyWhenFull
		Emit the intermediate results if the buffer is full



wordsStream
	.groupByKey()
	.windowedBy(hoppingWindow)
	.count()
	.suppress(Suppressed
			.untilWindowCloses(Suppressed.BufferConfig.unbounded().shutDownWhenFull())
	);


these intervals whatever the aggregation that's happening is emitting the results only after the commit.interval.ms is exhausted.

 
var windowsSize = Duration.ofSeconds(5);
var timeWindow = TimeWindows.ofSizeWithNoGrace(windowSize);

wordsStream
	.groupByKey()
	.windowedBy(timeWindow)
	.count()
	.suppress(Suppressed
			.untilWindowCloses(Suppressed.BufferConfig.unbounded().shutDownWhenFull())
	);
	

here we want to use
Suppressed.untilWindowCloses because we want to emit results anytime the window closes or exhausted in this case that window is 5seconds that we defined
here 
var windowsSize = Duration.ofSeconds(5);
var timeWindow = TimeWindows.ofSizeWithNoGrace(windowSize);

Suppressed.BufferConfig.unbounded() :::: this is recommended option an unbounded memory to hold the records

shutDownWhenFull() :::: if the buffer is full what action we are going to take is set by this i.e. shutdown the app if the buffer is full.






4. Windowing in Kafka Streams - Hopping Windows::::::::::::

Hopping Windows::::
	Hopping windows are fixed time windows that may overlap.
	Time Windows/Buckets are created from the clock of the app's running machine


Duration windowsSize = Duration.ofSeconds(5);
Duration advanceBySize = Duration.ofSeconds(3);

TimeWindows hoppingWindow = TimeWindows
				.ofSizeWithNoGrace(windowSize)
				.advanceBy(advanceBySize);

Here windowSize is 5 seconds for window buckets first window bucket is created with 5 second window all the aggregations based on the keys are performed in that
5 second window and next window bucket is overlapping window where start of the window is going to be 3 seconds from the starting window of the first window's start time
and the reason for that is we set the advanceBySize and the process continues for the remaining window buckets 

This is overlapping window so the  records are going to be overlapped between window buckets

if we have usecase where we wana perform aggregation and create the window buckets like this in those scenarios we can use Hopping Windows.








5. Windowing in Kafka Streams - Sliding Windows

Sliding Windows::
This is a fixed time, but the windows created are not defined by the machines running clock. ( the windows are derived from the event not derived from the clock time of machine)
Its the timestamp in the event that creates the window.
Windows that are created overlap with one another.(if there are events within defined window)

SlidingWindows slidingWindow = SlidingWindows.
					ofTimeDifferenceWithNoGrace(Duration.ofSeconds(5));

start time and end time of these window bucket are inclusive.which means if we receive record at the second of the starting window at the second of the end window those will be included
as part of aggregation.

use the Sliding Window when we have the usecase to create window in small increments of time.
another usecase is if we are not reliant on the machines clock time and we want the events to drive the actual time windows then in those kind of scenarios we can use Sliding Windows


========================================================================================================================


Aggregate the number of orders by grouping them in time windows
Aggregate the revenue of the orders by grouping them in time windows 
Group the aggreations by 15 seconds 

orderedDateTime

converting LocalDateTime to instant i.e. from IST to GMT 




















































